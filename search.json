[
  {
    "objectID": "MexLEF2023/index.html#sobre-esta-charla",
    "href": "MexLEF2023/index.html#sobre-esta-charla",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "Sobre esta charla",
    "text": "Sobre esta charla\nTellez, E. S., Moctezuma, D., Miranda, S., Graff, M., & Ruiz, G. (2023). Regionalized models for Spanish language variations based on Twitter. Language Resources and Evaluation, 1-31.\n@article{regionalized2023,\n  title={Regionalized models for Spanish language variations based on Twitter},\n  author={Tellez, Eric S and Moctezuma, Daniela and Miranda, Sabino and Graff, Mario and Ruiz, Guillermo},\n  journal={Language Resources and Evaluation},\n  pages={1--31},\n  year={2023},\n  publisher={Springer}\n}\n\nArXiv preprint."
  },
  {
    "objectID": "MexLEF2023/index.html#aplicaciones-de-procesamiento-de-lenguaje-natural-12",
    "href": "MexLEF2023/index.html#aplicaciones-de-procesamiento-de-lenguaje-natural-12",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "Aplicaciones de Procesamiento de Lenguaje Natural (1/2)",
    "text": "Aplicaciones de Procesamiento de Lenguaje Natural (1/2)\n\nMiner√≠a de opini√≥n\n\npositivo :)  ‚Äî neutro :|  ‚Äî negativo :( \n\nAn√°lisis de t√≥picos\nCarga emotiva de un mensaje: enojo, anticipaci√≥n, disgusto, miedo, gozo, tristeza, sorpresa, confianza.\nIdentificaci√≥n de humor\nIdentificaci√≥n de lenguaje de odio\nEtc."
  },
  {
    "objectID": "MexLEF2023/index.html#aplicaciones-de-procesamiento-de-lenguaje-natural-22",
    "href": "MexLEF2023/index.html#aplicaciones-de-procesamiento-de-lenguaje-natural-22",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "Aplicaciones de Procesamiento de Lenguaje Natural (2/2)",
    "text": "Aplicaciones de Procesamiento de Lenguaje Natural (2/2)\n\nPredicci√≥n indicadores socio-demogr√°ficos de usuarios de redes, e.g., edad, sexo, lugar de procedencia, ocupaci√≥n\nIdentificaci√≥n de autor√≠a: ¬øqui√©nes escriben?, ¬øc√≥mo escriben?, ¬øsobre qu√© escriben?\nEntender como se comportan usuarios, ¬øqu√© desean?, ¬øpor qu√©?\nMedici√≥n de discurso de odio en redes sociales, e.g., xenofobia, racismo, misoginia, cyberbulling.\nIdentificaci√≥n de posibles trastornos mentales, e.g., ansiedad, depresi√≥n, adicciones.\nAplicaciones a seguridad, salud, pol√≠ticas p√∫blicas, econom√≠a y finanzas."
  },
  {
    "objectID": "MexLEF2023/index.html#algunos-retos",
    "href": "MexLEF2023/index.html#algunos-retos",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "Algunos retos",
    "text": "Algunos retos\n\nEscritos informales: muchos errores, onomatopeya, importaci√≥n de t√©rminos, variaciones regionales, emojis, entre muchos otros.\nContextos cortos, conocimiento del mundo.\nNegaci√≥n, sarcasmo, iron√≠a, humor.\nSem√°ntica.\nRecursos ling√º√≠sticos reducidos para lenguajes diferentes del ingl√©s.\nMultimedios."
  },
  {
    "objectID": "MexLEF2023/index.html#regionalizaci√≥n",
    "href": "MexLEF2023/index.html#regionalizaci√≥n",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "Regionalizaci√≥n",
    "text": "Regionalizaci√≥n\n\nTodo lo anterior puede regionalizarse, ya que un mismo lenguaje puede usarse de manera diferente en diferentes regiones.\n\n\nPara tareas d√≥nde haya una fuerte carga cultural o de idiosincrasia el uso de recursos regionalizados es provechoso."
  },
  {
    "objectID": "MexLEF2023/index.html#regionalizaci√≥n-como-herramienta",
    "href": "MexLEF2023/index.html#regionalizaci√≥n-como-herramienta",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "Regionalizaci√≥n como herramienta",
    "text": "Regionalizaci√≥n como herramienta\nEntender las variaciones del lenguaje en las redes sociales es primordial ya que los mensajes suelen ser informales, y es com√∫n que los usuarios solo quieran ser le√≠dos por su c√≠rculo de personas cercanas."
  },
  {
    "objectID": "MexLEF2023/index.html#espa√±a",
    "href": "MexLEF2023/index.html#espa√±a",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "Espa√±a üá™üá∏",
    "text": "Espa√±a üá™üá∏\n\n\nme dais ascooooikiiikioooooooooooooooooooooooooo\nkina √±efla\nns cmo s exribe\no indeciso, nse ya x dnde cogerte colega\nq os follennjajabya quisieran\nen el metro q voy esta potando uno\n_USR üòÇüò≠üíî‚òπÔ∏èüò∞ pero por qu√© churra"
  },
  {
    "objectID": "MexLEF2023/index.html#argentina",
    "href": "MexLEF2023/index.html#argentina",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "Argentina üá¶üá∑",
    "text": "Argentina üá¶üá∑\n\n\npofr suerxte m8√≠s amigo mo son psic√≥patassa\npal pinnngooo\n_USR estos rompen todo! y la esposa del chorro me tir√≥ en la cara q era planera, 5 hijos tiene. me grita: vos segu√≠ alquilando! dec√≠ q no la agarro de los pelos x mi hijo q no le gusta el bardo.\ny dsp se comi√≥ un asado, moooy booenoüëåüëåü§£üòÇ\nmi hno se pone re denso no lo banco"
  },
  {
    "objectID": "MexLEF2023/index.html#m√©xico",
    "href": "MexLEF2023/index.html#m√©xico",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "M√©xico üá≤üáΩ",
    "text": "M√©xico üá≤üáΩ\n\n\n_USR ahora si! #achingarasumadre nefasto, corrupto y ratero, por mucho eres el peor alcalde que ha tenido _USR\nya me ando echando la primera ca** del a√±o\n_USR ac√° ya andaban con ‚Äúla chica que so√±√©‚Äù\n_USR ¬øno se te olvid√≥ ponerte calzones rojos hoy, verdad?\nun minuto de silencio por los que se estan reventando los dedos y las manos con los cohetes !!!"
  },
  {
    "objectID": "MexLEF2023/index.html#section-1",
    "href": "MexLEF2023/index.html#section-1",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "",
    "text": "Twitter corpora\n\n\n\n\n\n\ncountry\ncode\n#users\n#tweets\n#tokens\n\n\n\n\nArgentina\nAR\n1,376K\n234.22M\n2,887.92M\n\n\nBolivia\nBO\n36K\n1.15M\n20.99M\n\n\nChile\nCL\n415K\n45.29M\n719.24M\n\n\nColombia\nCO\n701K\n61.54M\n918.51M\n\n\nCosta Rica\nCR\n79K\n7.51M\n101.67M\n\n\nCuba\nCU\n32K\n0.37M\n6.30M\n\n\nDominican Republic\nDO\n112K\n7.65M\n122.06M\n\n\nEcuador\nEC\n207K\n13.76M\n226.03M\n\n\nEl Salvador\nSV\n49K\n2.71M\n44.46M\n\n\nEquatorial Guinea\nGQ\n1K\n8.93K\n0.14M\n\n\nGuatemala\nGT\n74K\n5.22M\n75.79M\n\n\nHonduras\nHN\n35K\n2.14M\n31.26M\n\n\nMexico\nMX\n1,517K\n115.53M\n1,635.69M\n\n\n\n\n\n\n\ncountry\ncode\n#users\n#tweets\n#tokens\n\n\n\n\nNicaragua\nNI\n35K\n3.34M\n42.47M\n\n\nPanama\nPA\n83K\n6.62M\n108.74M\n\n\nParaguay\nPY\n106K\n10.28M\n141.75M\n\n\nPeru\nPE\n271K\n15.38M\n241.60M\n\n\nPuerto Rico\nPR\n18K\n0.58M\n7.64M\n\n\nSpain\nES\n1,278K\n121.42M\n1,908.07M\n\n\nUruguay\nUY\n157K\n30.83M\n351.81M\n\n\nVenezuela\nVE\n421K\n35.48M\n556.12M\n\n\n-\n-\n-\n-\n-\n\n\nBrazil\nBR\n1,604K\n27.20M\n142.22M\n\n\nCanada\nCA\n149K\n1.55M\n21.58M\n\n\nFrance\nFR\n292K\n2.43M\n27.73M\n\n\nGreat Britain\nGB\n380K\n2.68M\n34.62M\n\n\nUnited States of America\nUS\n2,652K\n40.83M\n501.86M\n\n\nTotal\n\n12M\n795.74M\n10,876.25M\n\n\n\n\n\nSe colectaron mensajes georeferenciados de 2016 a 2019 usando el API de stream p√∫blico de Twitter."
  },
  {
    "objectID": "MexLEF2023/index.html#section-2",
    "href": "MexLEF2023/index.html#section-2",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "",
    "text": "Vocabularios\nWord embeddings\nModelos de lenguaje"
  },
  {
    "objectID": "MexLEF2023/index.html#vocabularios",
    "href": "MexLEF2023/index.html#vocabularios",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "Vocabularios",
    "text": "Vocabularios\n\n\n\n\n\n\n\n\n\n\n\n\ntoken\noccs\nndocs\nweight\nn_regions\ncountry_codes\n\n\n\n\nchau\n409,398\n387,910\n10.001527\n25\nAR:BO:BR:CA:CL:CO:CR:CU:DO:EC:ES:FR:GB:GT:HN:MX:NI:PA:PE:PR:PY:SV:US:UY:VE\n\n\nsiento\n2,830,971\n2,764,088\n7.168519\n26\nAR:BO:BR:CA:CL:CO:CR:CU:DO:EC:ES:FR:GB:GQ:GT:HN:MX:NI:PA:PE:PR:PY:SV:US:UY:VE\n\n\nadios\n341,841\n327,723\n10.244771\n25\nAR:BO:BR:CA:CL:CO:CR:CU:DO:EC:ES:FR:GB:GT:HN:MX:NI:PA:PE:PR:PY:SV:US:UY:VE\n\n\npersonas\n3,720,458\n3,620,029\n6.779321\n26\nAR:BO:BR:CA:CL:CO:CR:CU:DO:EC:ES:FR:GB:GQ:GT:HN:MX:NI:PA:PE:PR:PY:SV:US:UY:VE\n\n\nhuevo\n540,941\n527,792\n9.55728\n25\nAR:BO:BR:CA:CL:CO:CR:CU:DO:EC:ES:FR:GB:GT:HN:MX:NI:PA:PE:PR:PY:SV:US:UY:VE\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶"
  },
  {
    "objectID": "MexLEF2023/index.html#propiedades-de-los-vocabularios-12",
    "href": "MexLEF2023/index.html#propiedades-de-los-vocabularios-12",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "Propiedades de los vocabularios (1/2)",
    "text": "Propiedades de los vocabularios (1/2)\nLey de Zipf\n\nLey de Zipf para diferentes regiones (Twitter)"
  },
  {
    "objectID": "MexLEF2023/index.html#propiedades-de-los-vocabularios-22",
    "href": "MexLEF2023/index.html#propiedades-de-los-vocabularios-22",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "Propiedades de los vocabularios (2/2)",
    "text": "Propiedades de los vocabularios (2/2)\nLey de Heaps\n\n\nLey de Heaps para diferentes regiones (Twitter)"
  },
  {
    "objectID": "MexLEF2023/index.html#similitud-l√©xica-entre-regiones-12",
    "href": "MexLEF2023/index.html#similitud-l√©xica-entre-regiones-12",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "Similitud l√©xica entre regiones (1/2)",
    "text": "Similitud l√©xica entre regiones (1/2)"
  },
  {
    "objectID": "MexLEF2023/index.html#similitud-l√©xica-entre-regiones-22",
    "href": "MexLEF2023/index.html#similitud-l√©xica-entre-regiones-22",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "Similitud l√©xica entre regiones (2/2)",
    "text": "Similitud l√©xica entre regiones (2/2)"
  },
  {
    "objectID": "MexLEF2023/index.html#section-4",
    "href": "MexLEF2023/index.html#section-4",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "",
    "text": "Creados con fastText sobre nuestra corpora regionalizada de Twitter. Disponibles desde el sitio del proyecto en diferentes dimensiones, e.g, 300, 32, 16 y 8 dimensiones.\nModelos para las 27 regiones (26 pa√≠ses + un modelo aglutinado)"
  },
  {
    "objectID": "MexLEF2023/index.html#similitud-entre-regiones",
    "href": "MexLEF2023/index.html#similitud-entre-regiones",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "Similitud entre regiones",
    "text": "Similitud entre regiones\n\nNecesitamos una matriz de afinidad, como lo hicimos con el caso l√©xico, pero los embeddings de diferentes regiones no son comparables entre s√≠.\nPara calcular la similitud usamos una representaci√≥n que extrae informaci√≥n sem√°ntica basada en el grafo de los \\(k\\) vecinos cercanos. Esto soluciona el problema de comparaci√≥n, pero aumenta los requerimientos de computo."
  },
  {
    "objectID": "MexLEF2023/index.html#notas-para-el-modelado-de-regiones-con-word-embeddings",
    "href": "MexLEF2023/index.html#notas-para-el-modelado-de-regiones-con-word-embeddings",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "Notas para el modelado de regiones con word embeddings",
    "text": "Notas para el modelado de regiones con word embeddings\n\n\nRecordando, cada palabra es un punto en \\(300\\) dimensiones.\nLa distancia entre puntos se mide usando \\(1 - cos(u, v)\\).\nCada regi√≥n es una nube de puntos (palabras).\nUso de un vocabulario restringuido (preservar tokens que aparecen en al menos 10 regiones); esto tambi√©n reduce el problema de la disparidad de datos entre diferentes corpus.\nUso de herramientas especiales para la construcci√≥n de la gr√°fica de \\(k\\) vecinos (\\(k=33\\)).\nCada regi√≥n se representa en un vector disperso de muy alta dimensi√≥n, i.e., \\(10^{10}\\) componentes con \\(\\approx{}3.8\\) millones de componentes diferentes de zero. Se mide nuevamente con distancia coseno, pero con los vectores resultantes."
  },
  {
    "objectID": "MexLEF2023/index.html#ejemplos-de-k-vecinos-violencia",
    "href": "MexLEF2023/index.html#ejemplos-de-k-vecinos-violencia",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "Ejemplos de \\(k\\) vecinos (violencia)",
    "text": "Ejemplos de \\(k\\) vecinos (violencia)\n\n\n\n\n\nAR\n\n\nCL\n\n\nCO\n\n\nES\n\n\nMX\n\n\nUS\n\n\nVE\n\n\n\n\n\n\nviolencias\n\n\nviolencias\n\n\nviolencias\n\n\nviolencias\n\n\nviolencias\n\n\nviolencias\n\n\nviolencias\n\n\n\n\nagresion\n\n\nvandalismo\n\n\nviolentos\n\n\nviolente\n\n\nfeminicidios\n\n\nviolenten\n\n\nviolentos\n\n\n\n\ndenigracion\n\n\nagresion\n\n\nestigmatizacion\n\n\nintrafamiliar\n\n\nfemicidios\n\n\nviolente\n\n\nviolenten\n\n\n\n\nagresiones\n\n\nrepresion\n\n\nviolente\n\n\nrepresion\n\n\nviolentas\n\n\nviolentos\n\n\nviolente\n\n\n\n\nverbal\n\n\nbandalismo\n\n\nintrafamiliar\n\n\nviolenten\n\n\nvandalismo\n\n\nviolentas\n\n\nviolenta\n\n\n\n\nviolecia\n\n\nagresiones\n\n\nviolenten\n\n\nviolecia\n\n\nfemicidio\n\n\nviolentar\n\n\nconfrontacion\n\n\n\n\ndiscriminacion\n\n\nviolenten\n\n\nbandalismo\n\n\nviolentos\n\n\nviolente\n\n\nviolentan\n\n\nviolentas\n\n\n\n\nviolentos\n\n\nviolente\n\n\nagresiones\n\n\nviolentas\n\n\nbandalismo\n\n\ndelincuencia\n\n\nterrorismo\n\n\n\n\nmisoginia\n\n\nvandalismos\n\n\nviolentas\n\n\nviolenta\n\n\nfeminicidio\n\n\nviolentada\n\n\nintolerancia\n\n\n\n\nvandalismo\n\n\nviolentos\n\n\nrepresion\n\n\nviolentar\n\n\nagresion\n\n\nviolenta\n\n\nviolecia\n\n\n\n\nrevictimizacion\n\n\nviolenta\n\n\nvandalismo\n\n\nterrorismo\n\n\nintrafamiliar\n\n\nfeminicidios\n\n\nvandalismos\n\n\n\n\namedrentamiento\n\n\nviolentas\n\n\nintolerancia\n\n\ncriminalizacion\n\n\nasesinatos\n\n\nviolentado\n\n\nrepresion\n\n\n\n\ngenero\n\n\nincitacion\n\n\nincitacion\n\n\nviolentada\n\n\nagresiones\n\n\nvandalismo\n\n\nconfrontaciones\n\n\n\n\nintolerancia\n\n\nintrafamiliar\n\n\nintimidacion\n\n\nagresiones\n\n\ndelincuencia\n\n\nviolentando\n\n\nvandalismo"
  },
  {
    "objectID": "MexLEF2023/index.html#ejemplos-de-k-vecinos-chile",
    "href": "MexLEF2023/index.html#ejemplos-de-k-vecinos-chile",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "Ejemplos de \\(k\\) vecinos (chile)",
    "text": "Ejemplos de \\(k\\) vecinos (chile)\n\n\n\n\n\nAR\n\n\nCL\n\n\nCO\n\n\nES\n\n\nMX\n\n\nUS\n\n\nVE\n\n\n\n\n\n\necuador\n\n\nchiles\n\n\nuruguay\n\n\nchile0\n\n\nchile0\n\n\nchilen\n\n\nargentina\n\n\n\n\nperu\n\n\nchilee\n\n\nargentina\n\n\nargentina\n\n\nchilee\n\n\nchilee\n\n\nperu\n\n\n\n\nchile0\n\n\nchilito\n\n\nperu\n\n\nparaguay\n\n\npiquin\n\n\nchilenos\n\n\necuador\n\n\n\n\nuruguay\n\n\npais\n\n\nbolivia\n\n\nmexico\n\n\nchilito\n\n\nchile0\n\n\nchile0\n\n\n\n\nmexico\n\n\nlatinoamerica\n\n\nchilen\n\n\nuruguay\n\n\nchilensis\n\n\nchileno\n\n\nchiles\n\n\n\n\nchilen\n\n\nchile0\n\n\necuador\n\n\nperu\n\n\nhabanero\n\n\nchilena\n\n\nparaguay\n\n\n\n\nbrasil\n\n\nchil\n\n\nparaguay\n\n\nbolivia\n\n\nmorron\n\n\nuchile\n\n\nchilena\n\n\n\n\nbolivia\n\n\namerica0\n\n\nbrasil\n\n\ncolombia\n\n\nuchile\n\n\nshile\n\n\nbolivia\n\n\n\n\ncolombia\n\n\nüá®\n\n\nchilenos\n\n\nbrasil\n\n\nchiles\n\n\nchilito\n\n\nuruguay\n\n\n\n\nparaguay\n\n\nsuramerica\n\n\nmexico\n\n\nchilenos\n\n\nverdolagas\n\n\nchilenito\n\n\nbrasil\n\n\n\n\nchiles\n\n\nsudamerica\n\n\nchiles\n\n\nchilen\n\n\njalapeno\n\n\nargentina\n\n\nchilen\n\n\n\n\nchilee\n\n\nüá±\n\n\nhonduras\n\n\necuador\n\n\npimiento\n\n\nchilenas\n\n\nchilenos\n\n\n\n\nrusia\n\n\nargentina\n\n\nchile0\n\n\nchileno\n\n\nchil\n\n\nchiles\n\n\nchilensis"
  },
  {
    "objectID": "MexLEF2023/index.html#umap-projection-mx",
    "href": "MexLEF2023/index.html#umap-projection-mx",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "UMAP projection (MX)",
    "text": "UMAP projection (MX)"
  },
  {
    "objectID": "MexLEF2023/index.html#umap-projection-all",
    "href": "MexLEF2023/index.html#umap-projection-all",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "UMAP projection (ALL)",
    "text": "UMAP projection (ALL)"
  },
  {
    "objectID": "MexLEF2023/index.html#section-6",
    "href": "MexLEF2023/index.html#section-6",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "",
    "text": "Predicci√≥n emojis que podr√≠an estar asociados a un mensaje\n\n\nTomamos mensajes georeferenciados enero-febrero 2020.\nSeleccionamos los emoji objetivos entre los m√°s populares en espa√±ol: ü•∫, ‚ù§, üëå, üëè, üíî, üòÑ, üòä, üòå, üòç, üòí, üòò, üò°, üò¢, üò≠, ü§î.\nPara cada regi√≥n seleccionamos mensajes con solo uno de estos emoji.\nEl emoji en cuesti√≥n se reemplaza por la cadena _emo y se usa como etiqueta.\nPartici√≥n 50-50 por cada regi√≥n, se reporta micro recall.\nUsamos fastText supevisado con cada uno de los modelos regionales como preentrenados ‚Äì all vs all."
  },
  {
    "objectID": "MexLEF2023/index.html#section-7",
    "href": "MexLEF2023/index.html#section-7",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "",
    "text": "emoji\nAR\nBR\nCA\nCL\nCO\nCR\nCU\nDO\nEC\nES\nFR\nGB\nGT\nHN\nMX\nNI\nPA\nPE\nPY\nSV\nUS\nUY\nVE\n\n\n\n\nü•∫\n3749\n275\n158\n1263\n4760\n425\n26\n458\n745\n2640\n53\n95\n666\n708\n10160\n736\n827\n739\n684\n278\n2536\n446\n359\n\n\n‚ù§\n18454\n1775\n124\n2911\n6237\n821\n41\n348\n1123\n8394\n153\n255\n653\n609\n9508\n491\n880\n968\n1800\n219\n3216\n1478\n784\n\n\nüëå\n2114\n92\n34\n1016\n1455\n242\n20\n167\n253\n1825\n41\n58\n376\n157\n2754\n159\n313\n160\n666\n124\n597\n190\n71\n\n\nüëè\n6785\n433\n80\n2995\n2486\n159\n33\n369\n886\n7061\n64\n105\n197\n159\n4175\n52\n388\n577\n761\n222\n1281\n1248\n530\n\n\nüíî\n4278\n100\n28\n390\n1407\n161\n7\n123\n200\n1164\n26\n37\n134\n91\n1438\n105\n260\n115\n428\n31\n667\n283\n142\n\n\nüòÑ\n798\n26\n8\n301\n300\n42\n3\n56\n83\n776\n4\n18\n48\n48\n549\n17\n58\n115\n92\n53\n192\n85\n120\n\n\nüòä\n3609\n105\n35\n1803\n1475\n171\n32\n175\n364\n3827\n74\n56\n257\n129\n3395\n115\n236\n817\n369\n192\n829\n377\n293\n\n\nüòå\n1184\n70\n30\n827\n986\n132\n44\n280\n238\n1017\n23\n23\n123\n163\n2011\n151\n293\n248\n377\n78\n608\n167\n110\n\n\nüòç\n15999\n932\n111\n2190\n6824\n510\n28\n509\n837\n8000\n107\n136\n491\n514\n8711\n341\n1010\n999\n1996\n268\n2232\n1194\n686\n\n\nüòí\n3081\n89\n24\n920\n1718\n155\n50\n316\n291\n780\n18\n22\n163\n124\n2185\n175\n322\n213\n259\n127\n738\n359\n236\n\n\nüòò\n5935\n211\n70\n1764\n1482\n98\n29\n119\n347\n10785\n203\n99\n171\n72\n4290\n63\n136\n374\n190\n181\n1808\n719\n493\n\n\nüò°\n2777\n136\n60\n1098\n1412\n150\n5\n110\n291\n1320\n12\n25\n158\n52\n2428\n59\n155\n227\n250\n90\n769\n301\n252\n\n\nüò¢\n2144\n89\n38\n699\n1039\n153\n8\n102\n296\n1507\n31\n39\n151\n129\n2646\n131\n204\n271\n239\n69\n781\n227\n135\n\n\nüò≠\n13873\n436\n125\n1967\n4461\n581\n25\n604\n787\n3935\n157\n135\n388\n364\n6752\n321\n1057\n979\n1832\n200\n2799\n939\n530\n\n\nü§î\n6751\n275\n154\n2756\n4173\n440\n47\n614\n771\n4781\n99\n111\n421\n339\n7380\n211\n741\n1135\n937\n384\n1941\n951\n734"
  },
  {
    "objectID": "MexLEF2023/index.html#resultados-experimentales",
    "href": "MexLEF2023/index.html#resultados-experimentales",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "Resultados experimentales",
    "text": "Resultados experimentales\n\n\n\n\n\ncc\nminrecall\nmaxrecall\nlocalrank\ntop5\n\n\n\n\nAR\n0.4777\n0.49\n3\nUY,PY,AR,PE,CO\n\n\nBR\n0.4611\n0.4879\n1\nBR,ALL,DO,PY,CR\n\n\nCA\n0.2934\n0.3533\n18\nCL,ALL,CO,MX,US\n\n\nCL\n0.4257\n0.4494\n1\nCL,US,MX,AR,ES\n\n\nCO\n0.4247\n0.4365\n2\nUS,CO,VE,EC,GT\n\n\nCR\n0.3689\n0.388\n9\nUS,VE,ALL,MX,CO\n\n\nDO\n0.3383\n0.3813\n13\nUS,CO,VE,CL,ALL\n\n\nEC\n0.3797\n0.4138\n9\nMX,US,CL,ALL,CO\n\n\nES\n0.4754\n0.4855\n1\nES,AR,MX,US,VE\n\n\nFR\n0.4187\n0.4424\n4\nALL,GT,EC,FR,PA\n\n\n\n\n\n\n\ncc\nminrecall\nmaxrecall\nlocalrank\ntop5\n\n\n\n\nGB\n0.3467\n0.3756\n23\nALL,AR,ES,VE,MX\n\n\nGT\n0.3489\n0.3882\n13\nMX,US,ALL,CO,ES\n\n\nHN\n0.3354\n0.3671\n18\nPE,EC,BR,CR,UY\n\n\nMX\n0.4233\n0.4335\n1\nMX,GT,CR,US,CO\n\n\nNI\n0.3372\n0.3718\n18\nVE,CO,CL,MX,US\n\n\nPA\n0.3658\n0.3927\n10\nUS,CL,VE,CO,PE\n\n\nPE\n0.3799\n0.4196\n9\nMX,ALL,US,AR,CO\n\n\nPY\n0.4244\n0.4417\n1\nPY,US,BR,PE,UY\n\n\nSV\n0.3226\n0.3954\n18\nUS,CO,MX,CL,VE\n\n\nUS\n0.4041\n0.4244\n1\nUS,MX,CO,ES,CL\n\n\nUY\n0.4351\n0.4572\n1\nUY,US,CO,CL,VE\n\n\nVE\n0.3848\n0.4338\n4\nMX,CO,ES,VE,US"
  },
  {
    "objectID": "MexLEF2023/index.html#section-8",
    "href": "MexLEF2023/index.html#section-8",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "",
    "text": "Cuantiles de la variable localrank\n\n\n\nquantile\nrank\n\n\n\n\n0.00\n1.0\n\n\n0.25\n1.0\n\n\n0.50\n6.5\n\n\n0.75\n13\n\n\n1.00\n23\n\n\n\n\n¬øQu√© tan bueno es cada modelo? Apariciones en el top-5\n\n\n\n\ncc\nfreq\n\n\n\n\nUS\n17\n\n\nCO\n15\n\n\nMX\n13\n\n\nVE\n10\n\n\nALL\n9\n\n\nCL\n9\n\n\nES\n6\n\n\nAR\n5\n\n\n‚Ä¶\n‚Ä¶"
  },
  {
    "objectID": "MexLEF2023/index.html#average-rank-de-modelos",
    "href": "MexLEF2023/index.html#average-rank-de-modelos",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "average rank de modelos",
    "text": "average rank de modelos\n\n\n\n\n\nmodel\nvoc-size\navg. model rank\n\n\n\n\nUS\n292,465\n4.23\n\n\nCO\n324,635\n6.05\n\n\nMX\n438,136\n6.27\n\n\nCL\n282,737\n6.91\n\n\nVE\n271,924\n7.0\n\n\nALL\n1,696,232\n8.45\n\n\nPE\n178,113\n8.64\n\n\nUY\n200,032\n8.73\n\n\nEC\n147,560\n8.95\n\n\nAR\n673,424\n9.41\n\n\nES\n571,196\n10.95\n\n\nPY\n124,162\n11.14\n\n\n\n\n\n\n\nmodel\nvoc-size\navg. model rank\n\n\n\n\nBR\n127,205\n11.27\n\n\nCR\n103,086\n12.5\n\n\nPA\n111,635\n13.36\n\n\nGT\n95,252\n13.64\n\n\nDO\n108,655\n14.91\n\n\nGB\n82,418\n18.0\n\n\nNI\n68,605\n18.18\n\n\nFR\n69,843\n18.91\n\n\nCA\n63,161\n19.0\n\n\nSV\n73,833\n19.14\n\n\nHN\n60,580\n20.36"
  },
  {
    "objectID": "MexLEF2023/index.html#bert",
    "href": "MexLEF2023/index.html#bert",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "BERT",
    "text": "BERT\n\nLos modelos de lenguaje, Language Models (LM) utilizan el contexto de cada palabra para determinar su representaci√≥n.\nBERT es un modelo de lenguaje que en su momomento rompi√≥ el paradigma. Consiste en una serie de encoders que generan representaciones para cada palabra dependiendo de su contexto. El entrenamiento usa un lenguaje de enmascarado, Masked Language Model (MLM). Cada sentencia enmascar√° tokens de manera aleator√≠a (se enmascaran 15% de los tokens). Tambi√©n se entrena para predicci√≥n de la siguiente frase."
  },
  {
    "objectID": "MexLEF2023/index.html#recursos-computacionales-y-necesidad-de-datos",
    "href": "MexLEF2023/index.html#recursos-computacionales-y-necesidad-de-datos",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "Recursos computacionales y necesidad de datos",
    "text": "Recursos computacionales y necesidad de datos\n\n\nLos modelos de lenguaje requieren una gran cantidad de datos, solo generamos recursos con MLM sobre AR, CL, CO, MX, ES, UY, VE, y US, i.e., los m√°s grandes.\nTodos los modelos tienen series de dos encoders con cuatro cabezas de atenci√≥n cada una y una salida de 512 dimensiones por embedding\nCorresponde al small-size del BERT original, y es lo que actualmente podemos con los recursos que contamos en un tiempo pagable (usamos una estanci√≥n de trabajo con dos NVIDIA TITAN RTX con 24 GB cada una).\nNombramos a nuestro modelo BILMA por Bert In Latin America.\nUsamos un learning rate de \\(10^{-5}\\) con el optimizador Adam (usamos tensorflow 2 y Keras).\nLos modelos para CL, UY, VE, y US se entrenaron con 3 epocas y AR, CO, MX, y ES con solo una, dado los tama√±os de los corpus."
  },
  {
    "objectID": "MexLEF2023/index.html#bilma-vs-word-embeddings",
    "href": "MexLEF2023/index.html#bilma-vs-word-embeddings",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "BILMA vs word-embeddings",
    "text": "BILMA vs word-embeddings\n\n\n\n\n\nAccuracy en predicci√≥n de Emoji-15 - tuneado\n\n\n\n\n\n\n\ncc\nminrecall\nmaxrecall\nlocalrank\ntop5\n\n\n\n\nAR\n0.4777\n0.49\n3\nUY,PY,AR,PE,CO\n\n\nCL\n0.4257\n0.4494\n1\nCL,US,MX,AR,ES\n\n\nCO\n0.4247\n0.4365\n2\nUS,CO,VE,EC,GT\n\n\nES\n0.4754\n0.4855\n1\nES,AR,MX,US,VE\n\n\nMX\n0.4233\n0.4335\n1\nMX,GT,CR,US,CO\n\n\nUS\n0.4041\n0.4244\n1\nUS,MX,CO,ES,CL\n\n\nUY\n0.4351\n0.4572\n1\nUY,US,CO,CL,VE\n\n\nVE\n0.3848\n0.4338\n4\nMX,CO,ES,VE,US\n\n\n\nCon word-embeddings"
  },
  {
    "objectID": "MexLEF2023/index.html#section-9",
    "href": "MexLEF2023/index.html#section-9",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "",
    "text": "Se tune√≥ el modelo BILMA para predecir emoticones a√±adiendo dos capas lineales a los embeddings de inicio, por lo que se puede ver que se predice independiente de la posici√≥n.\nTuneado con 90%-10% del training set de la regi√≥n hasta que el accuracy converge.\nSe evalu√≥ con test regional.\nObserve que es una matriz de modelos pre-entrenados y tuneos.\nLos resultados en general son muy similares a los modelos de fastText, pero, los modelos BILMA pueden hacer m√°s cosas‚Ä¶"
  },
  {
    "objectID": "MexLEF2023/index.html#usando-bilma-para-completar-frases-mediante-m√°scaras",
    "href": "MexLEF2023/index.html#usando-bilma-para-completar-frases-mediante-m√°scaras",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "Usando BILMA para completar frases (mediante m√°scaras)",
    "text": "Usando BILMA para completar frases (mediante m√°scaras)\n\nAccuracy en la tarea MLM para el test"
  },
  {
    "objectID": "MexLEF2023/index.html#section-10",
    "href": "MexLEF2023/index.html#section-10",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "",
    "text": "MLM regional"
  },
  {
    "objectID": "MexLEF2023/index.html#trabajo-a-futuro",
    "href": "MexLEF2023/index.html#trabajo-a-futuro",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "Trabajo a futuro",
    "text": "Trabajo a futuro\n\n\nAplicaciones y medir impacto.\nModelos robustos para identificaci√≥n de regiones.\nReducir recursos computacionales.\n\nUso de un solo modelo con diferentes niveles de abstracci√≥n.\nCreaci√≥n de modelos menos costoso.\n\nEnriquecer los corpus m√°s peque√±os.\nOtros lenguajes (ingl√©s, ruso, √°rabe, etc‚Ä¶ o regionales con pocos datos).\n¬øSe puede usar Hausdorff u otras distancias para nubes de puntos para la comparaci√≥n de regiones?."
  },
  {
    "objectID": "MexLEF2023/index.html#preguntas",
    "href": "MexLEF2023/index.html#preguntas",
    "title": "Recursos regionalizados para el procesamiento autom√°tico de datos de redes sociales",
    "section": "¬øPreguntas?",
    "text": "¬øPreguntas?\n¬°Gracias por su atenci√≥n!\n\n\nModelos regionales del Espa√±ol:\n\nhttps://ingeotec.github.io/regional-spanish-models/\nhttps://github.com/INGEOTEC/regional-spanish-models/\n\n\nINGEOTEC:\n\nhttps://github.com/INGEOTEC/\nhttps://ingeotec.github.io/"
  },
  {
    "objectID": "TextClassification/poornima.html#ingeotec",
    "href": "TextClassification/poornima.html#ingeotec",
    "title": "Sentiment Analysis",
    "section": "INGEOTEC",
    "text": "INGEOTEC\n\n\nGitHub: https://github.com/INGEOTEC\nWebPage: https://ingeotec.github.io/",
    "crumbs": [
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "TextClassification/poornima.html#aguascalientes-m√©xico",
    "href": "TextClassification/poornima.html#aguascalientes-m√©xico",
    "title": "Sentiment Analysis",
    "section": "Aguascalientes, M√©xico",
    "text": "Aguascalientes, M√©xico",
    "crumbs": [
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "TextClassification/poornima.html#text-classification",
    "href": "TextClassification/poornima.html#text-classification",
    "title": "Sentiment Analysis",
    "section": "Text Classification",
    "text": "Text Classification\n\n\n\nDefinition\n\n\nThe aim is the classification of documents into a fixed number of predefined categories.\n\n\n\n\n\n\nPolarity\n\n\nEl d√≠a de ma√±ana no podr√© ir con ustedes a la librer√≠a\n\n\n\n\nNegative",
    "crumbs": [
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "TextClassification/poornima.html#text-classification-tasks",
    "href": "TextClassification/poornima.html#text-classification-tasks",
    "title": "Sentiment Analysis",
    "section": "Text Classification Tasks",
    "text": "Text Classification Tasks\n\n\n\nPolarity\n\n\nPositive, Negative, Neutral\n\n\n\n\n\n\nEmotion (Multiclass)\n\n\n\nAnger, Joy, ‚Ä¶\nIntensity of an emotion\n\n\n\n\n\n\n\nEvent (Binary)\n\n\n\nViolent\nCrime",
    "crumbs": [
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "TextClassification/poornima.html#profiling",
    "href": "TextClassification/poornima.html#profiling",
    "title": "Sentiment Analysis",
    "section": "Profiling",
    "text": "Profiling\n\n\n\nGender\n\n\nMan, Woman, Nonbinary, ‚Ä¶\n\n\n\n\n\n\nAge\n\n\nChild, Teen, Adult\n\n\n\n\n\n\nLanguage Variety\n\n\n\nSpanish: Spain, Cuba, Argentine, M√©xico, ‚Ä¶\nEnglish: United States, England, ‚Ä¶",
    "crumbs": [
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "TextClassification/poornima.html#machine-learning",
    "href": "TextClassification/poornima.html#machine-learning",
    "title": "Sentiment Analysis",
    "section": "Machine Learning",
    "text": "Machine Learning\n\n\n\nDefinition\n\n\nMachine learning (ML) is a subfield of artificial intelligence that focuses on the development and implementation of algorithms capable of learning from data without being explicitly programmed.\n\n\n\n\n\n\nTypes of ML algorithms\n\n\n\nUnsupervised Learning\nSupervised Learning\nReinforcement Learning",
    "crumbs": [
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "TextClassification/poornima.html#supervised-learning-multiclass",
    "href": "TextClassification/poornima.html#supervised-learning-multiclass",
    "title": "Sentiment Analysis",
    "section": "Supervised Learning (Multiclass)",
    "text": "Supervised Learning (Multiclass)",
    "crumbs": [
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "TextClassification/poornima.html#supervised-learning-binary",
    "href": "TextClassification/poornima.html#supervised-learning-binary",
    "title": "Sentiment Analysis",
    "section": "Supervised Learning (Binary)",
    "text": "Supervised Learning (Binary)",
    "crumbs": [
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "TextClassification/poornima.html#supervised-learning-classification",
    "href": "TextClassification/poornima.html#supervised-learning-classification",
    "title": "Sentiment Analysis",
    "section": "Supervised Learning (Classification)",
    "text": "Supervised Learning (Classification)\n\n\n                                                \n\n\n\n\n\n\nDecision function\n\n\n\n\n\\(g(\\mathbf x) = -0.78 x_1 + 0.60 x_2 + -0.88\\)",
    "crumbs": [
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "TextClassification/poornima.html#supervised-learning-geometry",
    "href": "TextClassification/poornima.html#supervised-learning-geometry",
    "title": "Sentiment Analysis",
    "section": "Supervised Learning (Geometry)",
    "text": "Supervised Learning (Geometry)\n\n\n                                                \n\n\n\n\n\nDecision function\n\n\n\n\n\\(g(\\mathbf x) = -0.78 x_1 + 0.60 x_2 + -0.88\\)",
    "crumbs": [
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "TextClassification/poornima.html#supervised-learning-geometry-2",
    "href": "TextClassification/poornima.html#supervised-learning-geometry-2",
    "title": "Sentiment Analysis",
    "section": "Supervised Learning (Geometry 2)",
    "text": "Supervised Learning (Geometry 2)\n\n\n                                                \n\n\n\n\n\n\\(w_0\\)\n\n\n\n\n\n\\(w_0 = -0.88\\)\n\\(w_0 = 0.88\\)",
    "crumbs": [
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "TextClassification/poornima.html#supervised-learning-geometry-3",
    "href": "TextClassification/poornima.html#supervised-learning-geometry-3",
    "title": "Sentiment Analysis",
    "section": "Supervised Learning (Geometry 3)",
    "text": "Supervised Learning (Geometry 3)\n\n\n                                                \n\n\n\n\n\nDecision function\n\n\n\n\n\n\\(g_{svm}(\\mathbf x) = -0.78 x_1 + 0.60 x_2 + -0.88\\)\n\\(g_{lr}(\\mathbf x) = -2.58 x_1 + 0.84 x_2 + -3.06\\)",
    "crumbs": [
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "TextClassification/poornima.html#training-set",
    "href": "TextClassification/poornima.html#training-set",
    "title": "Sentiment Analysis",
    "section": "Training set",
    "text": "Training set\n\n\n\n\n\n\n\n\n\n\ntext\nklass\n\n\n\n\n0\n¬øC√≥mo proteger nuestro negocio de un ciberataq...\n0\n\n\n1\nBALEAN A JOVEN AL INTERIOR DE LAS FONDAS DEL P...\n1\n\n\n2\n@RaulOrgaz3 @JuanGomezJurado @TodoJingles en e...\n0\n\n\n3\nCinco detenidos por disparos en escuela secund...\n1\n\n\n4\nMerece la pena destacar que a pesar de haber s...\n0\n\n\n5\nAl llegar, encuentran el cuerpo de la ni√±a qui...\n1\n\n\n6\nTipo 2 de la ma√±ana del d√≠a de hoy volv√≠amos c...\n0\n\n\n7\nFallece hombre atropellado por ruta 400 en ave...\n1",
    "crumbs": [
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "TextClassification/poornima.html#quiz",
    "href": "TextClassification/poornima.html#quiz",
    "title": "Sentiment Analysis",
    "section": "Quiz",
    "text": "Quiz\n\n\n\nQuestion\n\n\nWhich of the following tasks does the previous training set belong to?\n\nPolarity\nEmotion identification\nAggressive detection\nProfiling",
    "crumbs": [
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "TextClassification/poornima.html#training-set-2",
    "href": "TextClassification/poornima.html#training-set-2",
    "title": "Sentiment Analysis",
    "section": "Training set (2)",
    "text": "Training set (2)\n\n\n\nProblem\n\n\nThe independent variables are texts\n\n\n\n\n\n\n\nSolution\n\n\n\nRepresent the texts in an suitable format for the classifier\n\nToken as a vector\n\nSparse vector\nDense vector\n\nUtterance as a vector",
    "crumbs": [
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "TextClassification/poornima.html#token-as-vector",
    "href": "TextClassification/poornima.html#token-as-vector",
    "title": "Sentiment Analysis",
    "section": "Token as Vector",
    "text": "Token as Vector\n\n\n\nToken as vector\n\n\n\nThe idea is that each token \\(t\\) is associate to a vector \\(\\mathbf v_t \\in \\mathbb R^d\\)\nLet \\(\\mathcal V\\) represent the set composed by the different tokens\n\\(d\\) corresponds to the dimension of the vector\n\n\n\n\n\n\n\n\n\\(d &lt;&lt; \\lvert \\mathcal V \\rvert\\) (Dense Vector)\n\n\n\nGloVe\nWord2vec\nfastText",
    "crumbs": [
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "TextClassification/poornima.html#token-as-vector-2",
    "href": "TextClassification/poornima.html#token-as-vector-2",
    "title": "Sentiment Analysis",
    "section": "Token as Vector (2)",
    "text": "Token as Vector (2)\n\n\n\n\\(d = \\lvert \\mathcal V \\rvert\\) (Sparse Vector)\n\n\n\n\\(\\forall_{i \\neq j} \\mathbf v_i \\cdot \\mathbf v_j = 0\\)\n\\(\\mathbf v_i \\in \\mathbb R^d\\)\n\\(\\mathbf v_j \\in \\mathbb R^d\\)\n\n\n\n\n\n\n\n\nAlgorithm\n\n\n\nSort the vocabulary \\(\\mathcal V\\)\nAssociate \\(i\\)-th token to\n\\((\\ldots, 0, \\overbrace{\\beta_i}^i, 0, \\ldots)^\\intercal\\)\nwhere \\(\\beta_i &gt; 0\\)",
    "crumbs": [
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "TextClassification/poornima.html#utterance-as-vector",
    "href": "TextClassification/poornima.html#utterance-as-vector",
    "title": "Sentiment Analysis",
    "section": "Utterance as Vector",
    "text": "Utterance as Vector\n\n\n\nProcedure\n\n\n\\[\\mathbf x = \\sum_{t \\in \\mathcal U} \\mathbf{v}_t\\]\n\nwhere \\(\\mathcal{U}\\) corresponds to all the tokens of the utterance\nThe vector \\(\\mathbf{v}_t\\) is associated to token \\(t\\)\n\n\n\n\n\n\n\n\nUnit Vector\n\n\n\\[\\mathbf x = \\frac{\\sum_{t \\in \\mathcal U} \\mathbf v_t}{\\lVert \\sum_{t \\in \\mathcal U} \\mathbf v_t \\rVert} \\]",
    "crumbs": [
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "TextClassification/poornima.html#tokens",
    "href": "TextClassification/poornima.html#tokens",
    "title": "Sentiment Analysis",
    "section": "Tokens",
    "text": "Tokens\n\n\n\n\n\nflowchart LR\n    Entrada([Text]) --&gt;  Norm[Text Normalizer]\n    Norm --&gt; Seg[Tokenizer]\n    Seg --&gt; Terminos(...)\n\n\n\n\n\n\n\n\n\n\n\nText Normalization\n\n\n\nUser\nURL\nEntity\nCase sensitive\nPunctuation\nDiacritic\n\n\n\n\n\n\n\n\nDiacritic (remove)\n\n\n\nimport unicodedata\ntext = 'M√©xico'\noutput = \"\"\nfor x in unicodedata.normalize('NFD', text):\n    o = ord(x)\n    if 0x300 &lt;= o and o &lt;= 0x036F:\n        continue\n    output += x\noutput\n\n'Mexico'",
    "crumbs": [
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "TextClassification/poornima.html#text-normalization-1",
    "href": "TextClassification/poornima.html#text-normalization-1",
    "title": "Sentiment Analysis",
    "section": "Text Normalization",
    "text": "Text Normalization\n\n\n\nCase sensitive\n\n\n\ntext = \"M√©xico\"\noutput = text.lower()\noutput\n\n'm√©xico'\n\n\n\n\n\n\n\n\nUser (replace)\n\n\n\nimport re\ntext = \"go http://google.com, and find out\"\noutput = re.sub(r\"https?://\\S+\", \"_usr\", text)\noutput\n\n'go _usr and find out'",
    "crumbs": [
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "TextClassification/poornima.html#tokenizer",
    "href": "TextClassification/poornima.html#tokenizer",
    "title": "Sentiment Analysis",
    "section": "Tokenizer",
    "text": "Tokenizer\n\n\n\nCommon Types\n\n\n\nWords\nn-grams (Words)\nq-grams (Characters)\nskip-grams\n\n\n\n\n\n\n\n\nWords\n\n\n\ntext = 'I like playing football on Saturday'\nwords = text.split()\nwords\n\n['I', 'like', 'playing', 'football', 'on', 'Saturday']",
    "crumbs": [
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "TextClassification/poornima.html#tokenizer-2",
    "href": "TextClassification/poornima.html#tokenizer-2",
    "title": "Sentiment Analysis",
    "section": "Tokenizer (2)",
    "text": "Tokenizer (2)\n\n\n\n\n\nn-grams\n\n\n\ntext = 'I like playing football on Saturday'\nwords = text.split()\nn = 3\nn_grams = []\nfor a in zip(*[words[i:] for i in range(n)]):\n    n_grams.append(\"~\".join(a))\nn_grams\n\n['I~like~playing',\n 'like~playing~football',\n 'playing~football~on',\n 'football~on~Saturday']\n\n\n\n\n\n\n\n\n\nq-grams\n\n\n\ntext = 'I like playing'\nq = 4\nq_grams = []\nfor a in zip(*[text[i:] for i in range(q)]):\n    q_grams.append(\"\".join(a))\nq_grams\n\n['I li',\n ' lik',\n 'like',\n 'ike ',\n 'ke p',\n 'e pl',\n ' pla',\n 'play',\n 'layi',\n 'ayin',\n 'ying']",
    "crumbs": [
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "TextClassification/poornima.html#mu-tc",
    "href": "TextClassification/poornima.html#mu-tc",
    "title": "Sentiment Analysis",
    "section": "\\(\\mu\\)-TC",
    "text": "\\(\\mu\\)-TC\n\n\n\nTextModel\n\n\n\nfrom microtc import TextModel\nfrom microtc.params import OPTION_GROUP,\\\n  OPTION_DELETE, OPTION_NONE\ntm = TextModel(token_list=[-1],\n               num_option=OPTION_NONE,\n               usr_option=OPTION_DELETE,\n               url_option=OPTION_DELETE,\n               emo_option=OPTION_NONE,\n               lc=True,\n               del_dup=False,\n               del_punc=True,\n               del_diac=True)\n\n\n\n\n\n\ntext = 'I like playing football with @mgraffg'\ntm.tokenize(text)\n\n['i', 'like', 'playing', 'football', 'with']",
    "crumbs": [
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "TextClassification/poornima.html#mu-tc-2",
    "href": "TextClassification/poornima.html#mu-tc-2",
    "title": "Sentiment Analysis",
    "section": "\\(\\mu\\)-TC (2)",
    "text": "\\(\\mu\\)-TC (2)\n\n\n\n\n\nTextModel\n\n\n\ntm = TextModel(token_list=[-2, -1, 6],\n               num_option=OPTION_NONE,\n               usr_option=OPTION_DELETE,\n               url_option=OPTION_DELETE,\n               emo_option=OPTION_NONE, \n               lc=True, del_dup=False,\n               del_punc=True, del_diac=True)\n\n\n\n\n\n\ntext = 'I like playing...'\ntm.tokenize(text)\n\n['i~like',\n 'like~playing',\n 'i',\n 'like',\n 'playing',\n 'q:~i~lik',\n 'q:i~like',\n 'q:~like~',\n 'q:like~p',\n 'q:ike~pl',\n 'q:ke~pla',\n 'q:e~play',\n 'q:~playi',\n 'q:playin',\n 'q:laying',\n 'q:aying~']",
    "crumbs": [
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "TextClassification/poornima.html#training-set-1",
    "href": "TextClassification/poornima.html#training-set-1",
    "title": "Sentiment Analysis",
    "section": "Training set",
    "text": "Training set\n\nfrom EvoMSA.utils import Download\nfrom microtc.utils import tweet_iterator\nfrom os.path import isdir, isfile\nimport pandas as pd\nfrom random import shuffle\n\nURL = 'https://github.com/INGEOTEC/Delitos/releases/download/Datos/delitos.zip'\nif not isfile('delitos.zip'):\n  Download(URL,\n           'delitos.zip')\nif not isdir('delitos'):\n  !unzip -Pingeotec delitos.zip",
    "crumbs": [
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "TextClassification/poornima.html#utterance-as-vector-1",
    "href": "TextClassification/poornima.html#utterance-as-vector-1",
    "title": "Sentiment Analysis",
    "section": "Utterance as Vector",
    "text": "Utterance as Vector\n\n\n\n\n\nTextModel\n\n\n\ntm = TextModel(token_list=[-1],\n               num_option=OPTION_NONE,\n               usr_option=OPTION_DELETE,\n               url_option=OPTION_DELETE,\n               emo_option=OPTION_NONE, \n               lc=True, del_dup=False,\n               del_punc=True, del_diac=True)\n\n\n\n\n\n\n\n\nTokenizer\n\n\n\nfrom microtc.utils import tweet_iterator\nfname = 'delitos/delitos_ingeotec_Es_train.json'\ntraining_set = list(tweet_iterator(fname))\ntm.tokenize(training_set[0])[:3]\n\n['este', 'caso', 'tiene']\n\n\n\n\n\n\n\n\n\n\nVocabulary\n\n\n\nfrom microtc.utils import Counter\nvoc = Counter()\nfor text in training_set:\n  tokens = set(tm.tokenize(text))\n  voc.update(tokens)\nvoc.most_common(n=3)\n\n[('de', 980), ('en', 803), ('la', 653)]",
    "crumbs": [
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "TextClassification/poornima.html#utterance-as-vector-2",
    "href": "TextClassification/poornima.html#utterance-as-vector-2",
    "title": "Sentiment Analysis",
    "section": "Utterance as Vector (2)",
    "text": "Utterance as Vector (2)\n\n\n\nInverse Document Frequency (IDF)\n\n\n\ntoken2id = {}\ntoken2beta = {}\nN = np.log2(voc.update_calls)\nfor id, (k, n) in enumerate(voc.items()):\n  token2id[k] = id\n  token2beta[k] = N - np.log2(n)\n\n\n\n\n\n\n\n\nTerm Frequency - IDF\n\n\n\ntext = training_set[3]['text']\ntokens = tm.tokenize(text)\nvector = []\nfor token, tf in zip(*np.unique(tokens, return_counts=True)):\n  if token not in token2id:\n    continue\n  vector.append((token2id[token], tf * token2beta[token]))\nvector[:4]\n\n[(62, np.float64(7.906890595608518)),\n (59, np.float64(5.8479969065549495)),\n (10, np.float64(1.3269461696539864)),\n (3, np.float64(2.5277907564370796))]",
    "crumbs": [
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "TextClassification/poornima.html#utterance-as-vector-3",
    "href": "TextClassification/poornima.html#utterance-as-vector-3",
    "title": "Sentiment Analysis",
    "section": "Utterance as Vector (3)",
    "text": "Utterance as Vector (3)\n\n\n\n\\(\\mu\\)-TC\n\n\n\ntm.fit(training_set)\n\n&lt;microtc.textmodel.TextModel at 0x7f9e173a0a30&gt;\n\n\n\n\n\n\n\n\nUtterance as Vector\n\n\n\ntext = training_set[3]['text']\ntm[text][:4]\n\n[(3535, np.float64(0.08495635021337841)),\n (5135, np.float64(0.0766897986795882)),\n (6598, np.float64(0.3526199879358128)),\n (3350, np.float64(0.19654493631439243))]",
    "crumbs": [
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "TextClassification/poornima.html#quiz-1",
    "href": "TextClassification/poornima.html#quiz-1",
    "title": "Sentiment Analysis",
    "section": "Quiz",
    "text": "Quiz\n\n\n\nQuestion\n\n\nWhich of the following representations do you consider to produce a larger vocabulary?\n\n\n\n\n\n\n\n\nA\n\n\n\ntmA = TextModel(token_list=[-1, 3],\n                num_option=OPTION_NONE,\n                usr_option=OPTION_DELETE,\n                url_option=OPTION_DELETE,\n                emo_option=OPTION_NONE, \n                lc=True, del_dup=False,\n                del_punc=True,\n                del_diac=True\n               ).fit(training_set)\n\n\n\n\n\n\n\n\nB\n\n\n\ntmB = TextModel(token_list=[-1, 6],\n                num_option=OPTION_NONE,\n                usr_option=OPTION_DELETE,\n                url_option=OPTION_DELETE,\n                emo_option=OPTION_NONE, \n                lc=True, del_dup=False,\n                del_punc=True,\n                del_diac=True\n               ).fit(training_set)",
    "crumbs": [
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "TextClassification/poornima.html#procedure-1",
    "href": "TextClassification/poornima.html#procedure-1",
    "title": "Sentiment Analysis",
    "section": "Procedure",
    "text": "Procedure\n\n\n\nText as Vectors\n\n\n\nX = tm.transform(training_set)\n\n\n\n\n\n\n\n\nTraining a Classifier\n\n\n\nfrom sklearn.svm import LinearSVC\nlabels = [x['klass'] for x in training_set]\nm = LinearSVC(dual='auto').fit(X, labels)\n\n\n\n\n\n\n\n\n\nPredict a text\n\n\n\nX = tm.transform(['Buenos d√≠as']) # good morning\nm.predict(X)\n\narray([0])",
    "crumbs": [
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "TextClassification/poornima.html#performance",
    "href": "TextClassification/poornima.html#performance",
    "title": "Sentiment Analysis",
    "section": "Performance",
    "text": "Performance\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import recall_score\n\nkfold = KFold(n_splits=5, shuffle=True, random_state=1)\nperf = []\nfor tr, vs in kfold.split(training_set):\n    train = [training_set[i] for i in tr]\n    val = [training_set[i] for i in vs]\n    tm = TextModel(token_list=[-1], num_option=OPTION_NONE,\n                   usr_option=OPTION_DELETE, url_option=OPTION_DELETE,\n                   emo_option=OPTION_NONE, lc=True, del_dup=False,\n                   del_punc=True, del_diac=True).fit(train)\n    labels = [x['klass'] for x in train]\n    m = LinearSVC(dual='auto').fit(tm.transform(train), labels)\n    hy = m.predict(tm.transform(val))\n    _ = recall_score([x['klass'] for x in val], hy, average='macro')\n    perf.append(_)\nnp.mean(perf)\n\nnp.float64(0.8200309412585819)",
    "crumbs": [
      "Sentiment Analysis"
    ]
  },
  {
    "objectID": "TextClassification/educacion.html#aguascalientes-m√©xico",
    "href": "TextClassification/educacion.html#aguascalientes-m√©xico",
    "title": "Impacto del Procesamiento de Lenguaje Natural en la Educaci√≥n",
    "section": "Aguascalientes, M√©xico",
    "text": "Aguascalientes, M√©xico",
    "crumbs": [
      "Impacto del Procesamiento de Lenguaje Natural en la Educaci√≥n"
    ]
  },
  {
    "objectID": "TextClassification/educacion.html#definiciones",
    "href": "TextClassification/educacion.html#definiciones",
    "title": "Impacto del Procesamiento de Lenguaje Natural en la Educaci√≥n",
    "section": "Definiciones",
    "text": "Definiciones\n\n\n\n\n\nInteligencia Artificial (IA)\n\n\nConjunto de teor√≠as, m√©todos y algoritmos para el desarrollo y estudio de sistemas que presentan un comportamiento que ser√≠a identificado como inteligente.\n\n\n\n\n\n\n\nAprendizaje Computacional\n\n\nAprendizaje Computacional es una sub√°rea de Inteligencia Artificial que estudia el desarrollo e implementaci√≥n de algoritmos capaces de aprender de datos de manera aut√≥noma sin haber sido expl√≠citamente programados.\n\n\n\n\n\n\n\n\nProcesamiento de Lenguaje Natural\n\n\nConjunto de teor√≠as, m√©todos y algoritmos para el desarrollo y estudio de sistemas que permitan el entendimiento, generaci√≥n y manipulaci√≥n del lenguaje humano.",
    "crumbs": [
      "Impacto del Procesamiento de Lenguaje Natural en la Educaci√≥n"
    ]
  },
  {
    "objectID": "TextClassification/educacion.html#percepci√≥n-de-inteligencia-artificial-ia",
    "href": "TextClassification/educacion.html#percepci√≥n-de-inteligencia-artificial-ia",
    "title": "Impacto del Procesamiento de Lenguaje Natural en la Educaci√≥n",
    "section": "Percepci√≥n de Inteligencia Artificial (IA)",
    "text": "Percepci√≥n de Inteligencia Artificial (IA)\n\n\n\n\n\nAs√≠ la vemos\n\n\n\n\n\n\n\n\n\n\nAs√≠ est√°",
    "crumbs": [
      "Impacto del Procesamiento de Lenguaje Natural en la Educaci√≥n"
    ]
  },
  {
    "objectID": "TextClassification/educacion.html#tareas",
    "href": "TextClassification/educacion.html#tareas",
    "title": "Impacto del Procesamiento de Lenguaje Natural en la Educaci√≥n",
    "section": "Tareas",
    "text": "Tareas\n\n\n\nL. Chen, Chen, y Lin (2020)\n\n\n\nCalificar y dar retroalimentaci√≥n\nCreaci√≥n de planes de aprendizaje personalizados\nPredecir deserci√≥n\nEstilo de aprendizaje basado en informaci√≥n personal\nDescubrir dificultades de los estudiantes\nSelecci√≥n de cursos",
    "crumbs": [
      "Impacto del Procesamiento de Lenguaje Natural en la Educaci√≥n"
    ]
  },
  {
    "objectID": "TextClassification/educacion.html#tendencias",
    "href": "TextClassification/educacion.html#tendencias",
    "title": "Impacto del Procesamiento de Lenguaje Natural en la Educaci√≥n",
    "section": "Tendencias",
    "text": "Tendencias\n\n\n\nX. Chen et¬†al. (2022) menciona\n\n\n\nTutores Inteligentes - educaci√≥n especial\nProcesamiento de Lenguaje Natural - Ense√±anza del lenguaje\nMiner√≠a de datos - Predicci√≥n de rendimiento\nRedes neuronales - Evaluaci√≥n de ense√±anza\nComputaci√≥n afectiva - Detecci√≥n de emociones\nSistemas de recomendaci√≥n - Aprendizaje personalizado\n\n\n\n\n\n\n\n\nForos importantes (index H)\n\n\n\nInternational Journal of Artificial Intelligence in Education\nInternational Conference of Artificial Intelligence in Education\nComputers & Education",
    "crumbs": [
      "Impacto del Procesamiento de Lenguaje Natural en la Educaci√≥n"
    ]
  },
  {
    "objectID": "TextClassification/educacion.html#xxx",
    "href": "TextClassification/educacion.html#xxx",
    "title": "Impacto del Procesamiento de Lenguaje Natural en la Educaci√≥n",
    "section": "XxX",
    "text": "XxX\nZatarain Cabada, C√°rdenas L√≥pez, y Escalante (2023)",
    "crumbs": [
      "Impacto del Procesamiento de Lenguaje Natural en la Educaci√≥n"
    ]
  },
  {
    "objectID": "TextClassification/educacion.html#aplicaciones-en-la-educaci√≥n",
    "href": "TextClassification/educacion.html#aplicaciones-en-la-educaci√≥n",
    "title": "Impacto del Procesamiento de Lenguaje Natural en la Educaci√≥n",
    "section": "Aplicaciones en la Educaci√≥n",
    "text": "Aplicaciones en la Educaci√≥n\nPor hacer",
    "crumbs": [
      "Impacto del Procesamiento de Lenguaje Natural en la Educaci√≥n"
    ]
  },
  {
    "objectID": "IberLEF2023/short-davincis2023.html#ingeotec-research-group",
    "href": "IberLEF2023/short-davincis2023.html#ingeotec-research-group",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "INGEOTEC research group",
    "text": "INGEOTEC research group\n\n\n\n\nGitHub: https://github.com/INGEOTEC\nWebPage: https://ingeotec.github.io/",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/short-davincis2023.html#the-da-vincis-task-at-iberlef-2023",
    "href": "IberLEF2023/short-davincis2023.html#the-da-vincis-task-at-iberlef-2023",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "The DA-VINCIS Task at IberLEF 2023",
    "text": "The DA-VINCIS Task at IberLEF 2023\n\n\nAn open challenge to develop multimodal models to detect violent incidents on Twitter. The task has two tracks:\n\nviolent event identification and\nviolent event category recognition.\n\nThis presentation describes our solution system for the (i) track using only text-based features. More particularly, we used our EvoMSA framework (Graff et¬†al. 2020).",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/short-davincis2023.html#our-approach-evomsa-2.0",
    "href": "IberLEF2023/short-davincis2023.html#our-approach-evomsa-2.0",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Our approach: EvoMSA 2.0",
    "text": "Our approach: EvoMSA 2.0",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/short-davincis2023.html#our-text-representations",
    "href": "IberLEF2023/short-davincis2023.html#our-text-representations",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Our text representations",
    "text": "Our text representations\n\n\nSparse bag of words (SBOW)\nDense bag of words (DBOW)",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/short-davincis2023.html#sparse-bag-of-words",
    "href": "IberLEF2023/short-davincis2023.html#sparse-bag-of-words",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Sparse bag of words",
    "text": "Sparse bag of words\n\nThe text is preprocessed and tokenized, then each token \\(t\\) is associated with a vector \\(\\mathbf{v_t} \\in \\mathbb R^d\\) where the \\(i\\)-th component, i.e., \\(\\mathbf{v_t}_i\\), contains the Inverse-Document-Frequency (IDF) value of the token \\(t\\) and \\(\\forall_{j \\neq i} \\mathbf{v_t}_j=0\\).\n\nThe set of vectors \\(\\mathbf V = \\{ \\mathbf v_t \\}\\) corresponds to the vocabulary, and there are \\(|\\mathbf V| = d\\) different tokens in the vocabulary.",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/short-davincis2023.html#section",
    "href": "IberLEF2023/short-davincis2023.html#section",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "",
    "text": "A text is represented by the sequence of its tokens, i.e., \\((t_1, t_2, \\ldots)\\). The text is then vectorized as:\n\\[\n    \\textsf{sbow}(\\text{some text}) = \\textsf{sbow}((t_1, t_2, \\ldots)) = \\frac{\\sum_t \\mathbf{v_t}}{\\lVert \\sum_t \\mathbf{v_t} \\rVert}\n\\]",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/short-davincis2023.html#sbow",
    "href": "IberLEF2023/short-davincis2023.html#sbow",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "SBoW",
    "text": "SBoW",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/short-davincis2023.html#dense-bag-of-words",
    "href": "IberLEF2023/short-davincis2023.html#dense-bag-of-words",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Dense bag of words",
    "text": "Dense bag of words\n\n\nIn contrast to SBOW, the dense embeddings come from associating each component to the decision value of a text classifier (e.g., based on SBOW) pre-trained on a different collection of tweets. \nWithout loss of generality, it is assumed that there are \\(M\\) labeled datasets, each one contains a binary text classification problem.\n\n\nnoting that if a dataset has \\(K\\) labels, then this dataset can be represented as \\(K\\) binary classification problems following the one versus the rest approach, i.e., it is transformed to \\(K\\) datasets.",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/short-davincis2023.html#construction-of-the-dbow",
    "href": "IberLEF2023/short-davincis2023.html#construction-of-the-dbow",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Construction of the DBOW",
    "text": "Construction of the DBOW\n\n\nFor each of these \\(M\\) binary text classification problems, a SBOW-based classifier is built using a pre-trained SBOW representation and a linear Support Vector Machine (SVM) as the classifier. Consequently, there are \\(M\\) binary text classifiers, i.e., \\((c_1, c_2, \\ldots, c_M)\\). Additionally, the decision function of \\(c_i\\) is a value where the sign indicates the class.   The text representation is the vector obtained by concatenating the decision functions of the \\(M\\) classifiers and then normalizing the vector to have unitary norm.",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/short-davincis2023.html#dense-bow-stacking",
    "href": "IberLEF2023/short-davincis2023.html#dense-bow-stacking",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Dense BoW (stacking)",
    "text": "Dense BoW (stacking)\n\n\n\n\nStacking: All models aggregated are used according to their weights for producing an output, the final classification.",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/short-davincis2023.html#sbow-parameters-continued",
    "href": "IberLEF2023/short-davincis2023.html#sbow-parameters-continued",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "SBOW parameters (continued‚Ä¶)",
    "text": "SBOW parameters (continued‚Ä¶)\n\nThe pre-trained BoW is estimated from 4,194,304 (\\(2^{22}\\)) tweets randomly selected in a larger collection of messages.\nThe IDF values were estimated from the collections, and some tokens were selected from all the available ones found in the collection.",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/short-davincis2023.html#sbow-parameters-continued-1",
    "href": "IberLEF2023/short-davincis2023.html#sbow-parameters-continued-1",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "SBOW parameters (continued‚Ä¶)",
    "text": "SBOW parameters (continued‚Ä¶)\nTwo procedures were used to select the tokens:\n\nthe first corresponds to selecting the \\(d\\) tokens with the highest frequency, and the other to normalize the frequency w.r.t. their type, i.e., bigrams, words, and q-grams of characters. Once the frequency is normalized, one selects the \\(d\\) tokens with the highest normalized frequency.\nThe value of \\(d\\) is \\(2^{17}\\); however, one can also find in the library models for \\(2^{13}, 2^{14}, \\ldots, 2^{17}.\\)",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/short-davincis2023.html#sbow-parameters-continued-2",
    "href": "IberLEF2023/short-davincis2023.html#sbow-parameters-continued-2",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "SBOW parameters (continued‚Ä¶)",
    "text": "SBOW parameters (continued‚Ä¶)\n\nIt is also possible to train the BoW model using the training set; in this case, we used the default parameters. The only difference is that vocabulary size \\(d\\) is bounded by the training set tokens.",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/short-davincis2023.html#dbow-parameters-continued",
    "href": "IberLEF2023/short-davincis2023.html#dbow-parameters-continued",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "DBOW parameters (continued‚Ä¶)",
    "text": "DBOW parameters (continued‚Ä¶)\nFollowing an equivalent approach used in the development of the pre-trained BoW, different dense representations were created.\nThese correspond to varying the size of the vocabulary and the two procedures used to select the tokens. Vector spaces:\n\ndataset is in \\(\\mathbb R^{57}\\).\nemoji is in \\(\\mathbb R^{567}\\).\nkeyword is in \\(\\mathbb R^{2048}\\).",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/short-davincis2023.html#configurations-continued",
    "href": "IberLEF2023/short-davincis2023.html#configurations-continued",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Configurations (continued‚Ä¶)",
    "text": "Configurations (continued‚Ä¶)\nThe different configurations tested in this competition are described below. These configurations include BoW and a combination of BoW with dense representations. Stack generalization combines the different text classifiers, and the top classifier was a Naive Bayes algorithm. The specific implementation of this configuration can be seen in EvoMSA‚Äôs documentation.",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/short-davincis2023.html#configurations-continued-1",
    "href": "IberLEF2023/short-davincis2023.html#configurations-continued-1",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Configurations (continued‚Ä¶)",
    "text": "Configurations (continued‚Ä¶)\nThe list of configurations\n\n\nbow: Pre-trained BoW where the tokens are selected based on a normalized frequency w.r.t. its type, i.e., bigrams, words, and q-grams of characters.\nbow_voc_selection: Pre-trained BoW where the tokens correspond to the most frequent ones.\nbow_training_set: BoW trained with the training set; the number of tokens corresponds to all the tokens in the set.\nstack_bow_keywords_emojis: Stack generalization approach where the base classifiers are the BoW, the emojis, and the keywords dense BoW.\n\nstack_bow_keywords_emojis_voc_selection: Stack generalization approach where the base classifiers are the BoW, the emojis, and the keywords dense BoW. The tokens in these models were selected based on a normalized frequency w.r.t. its type, i.e., bigrams, words, and q-grams of characters.",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/short-davincis2023.html#configurations-continued-2",
    "href": "IberLEF2023/short-davincis2023.html#configurations-continued-2",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Configurations (continued‚Ä¶)",
    "text": "Configurations (continued‚Ä¶)\n\n\nstack_bows: Stack generalization approach where the base classifiers are BoW with the two token selection procedures described previously (i.e., bow and bow_voc_selection).\nstack_2_bow_keywords: Stack generalization approach where with four base classifiers. These correspond to two BoW and two dense BoW (emojis and keywords), where the difference in each is the procedure used to select the tokens, i.e., the most frequent or normalized frequency.\nstack_2_bow_tailored_keywords: Stack generalization approach with four base classifiers. These correspond to two BoW and two dense BoW (emojis and keywords), where the difference in each is the procedure used to select the tokens, i.e., the most frequent or normalized frequency. The second difference is that the dense representation with normalized frequency also includes models for the most discriminant words selected by a BoW classifier in the training set. We refer to these latter representations as tailored keywords.",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/short-davincis2023.html#configurations-continued-3",
    "href": "IberLEF2023/short-davincis2023.html#configurations-continued-3",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Configurations (continued‚Ä¶)",
    "text": "Configurations (continued‚Ä¶)\n\n\nstack_2_bow_all_keywords: Stack generalization approach with four base classifiers equivalent to stack_2_bow_keywords where the difference is that the dense representations include the models created with the human-annotated datasets.\nstack_2_bow_tailored_all_keywords: Stack generalization approach with four base classifiers equivalent to stack_2_bow_all_keywords, where the difference is that the dense representation with normalized frequency also includes the tailored keywords.\nstack_3_bows: Stack generalization approach with three base classifiers. All of them are BoW; the first two correspond pre-trained BoW with the two token selection procedures described previously (i.e., bow and bow_voc_selection), and the latest is a BoW trained on the training set (i.e., bow_training_set).",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/short-davincis2023.html#configurations-continued-4",
    "href": "IberLEF2023/short-davincis2023.html#configurations-continued-4",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Configurations (continued‚Ä¶)",
    "text": "Configurations (continued‚Ä¶)\n\n\nstack_3_bows_tailored_keywords: Stack generalization approach with five base classifiers. The first corresponds to a BoW trained on the training set, and the rest are used in stack_2_bow_tailored_keywords.\nstack_3_bow_tailored_all_keywords: Stack generalization approach with five base classifiers. It is comparable to stack_3_bows_tailored_keywords being the difference in the use of the tailored keywords.",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/short-davincis2023.html#competition",
    "href": "IberLEF2023/short-davincis2023.html#competition",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Competition",
    "text": "Competition\n\n\n\nPerformance comparison of our submission (INGEOTEC) and the competition‚Äôs winner in each edition. The best performance is in boldface.\n\n\n\nDA-VINCIS 2023\nDA-VINCIS 20222\n\n\n\n\nWinner\n0.9264\n0.7817\n\n\nINGEOTEC\n0.8903\n0.7510\n\n\nDifference\n4.1%\n4.1%\n\n\n\n\n\nAnother comparison that one can make with the Table‚Äôs data is computing the difference between the best performance in k-fold cross-validation and the worst; it can be observed that for the 2023 edition, the difference is 1.4%, and for 2022 is 13.3%. The difference of 1.4% indicates that following this approach will be complicated to improve. On the other hand, for the edition 2022, there might be room for improvement following the presented approach.",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/short-davincis2023.html#other-competitions-performance",
    "href": "IberLEF2023/short-davincis2023.html#other-competitions-performance",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Other competitions performance",
    "text": "Other competitions performance\n\nPerformance comparison. EvoMSA‚Äô results\n\n\nCompetition\nWinner\nEvoMSA 2.0\nDifference\n\n\n\n\nPoliticEs (Gender)\n0.8296\n0.7115\n16.6%\n\n\nPoliticEs (Profession)\n0.8608\n0.8379\n2.7%\n\n\nPoliticEs (Ideology Binary)\n0.8967\n0.8913\n0.6%\n\n\nPoliticEs (Ideology Multiclass)\n0.6913\n0.6694\n3.3%\n\n\nREST-MEX (Polarity)\n0.6216\n0.5548\n12.0%\n\n\nREST-MEX (Type)\n0.9903\n0.9805\n1.0%\n\n\nREST-MEX (Country)\n0.9420\n0.9270\n1.6%",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/short-davincis2023.html#other-competitions-performance-continued",
    "href": "IberLEF2023/short-davincis2023.html#other-competitions-performance-continued",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Other competitions performance (Continued‚Ä¶)",
    "text": "Other competitions performance (Continued‚Ä¶)\n\nPerformance comparison. EvoMSA‚Äô results\n\n\nCompetition\nWinner\nEvoMSA 2.0\nDifference\n\n\n\n\nHOMO-MEX\n0.8847\n0.8050\n9.9%\n\n\nHOPE (ES)\n0.9161\n0.4198\n118.2%\n\n\nHOPE (EN)\n0.5012\n0.4429\n13.2%\n\n\nDIPROMATS (ES)\n0.8089\n0.7485\n8.1%\n\n\nDIPROMATS (EN)\n0.8090\n0.7255\n11.5%\n\n\nHUHU\n0.820\n0.775\n5.8%",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/short-davincis2023.html#example",
    "href": "IberLEF2023/short-davincis2023.html#example",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Example",
    "text": "Example\n\nUsing a tailored dense model with the DA-VINCIS 2023 dataset, we can see:",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/short-davincis2023.html#example-1",
    "href": "IberLEF2023/short-davincis2023.html#example-1",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Example",
    "text": "Example\n\nUsing a dense model with the DA-VINCIS 2023 dataset, we can see:",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/short-davincis2023.html#example---performance-varying-d",
    "href": "IberLEF2023/short-davincis2023.html#example---performance-varying-d",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Example - performance varying \\(d\\)",
    "text": "Example - performance varying \\(d\\)\n\n\n\n\n\n\n\nMore details",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/short-davincis2023.html#conclusions-continued",
    "href": "IberLEF2023/short-davincis2023.html#conclusions-continued",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Conclusions (continued‚Ä¶)",
    "text": "Conclusions (continued‚Ä¶)\n\n\nExplainability of the model (with a simple bow outstanding results). Simplest solution Fast solution (in training and test), low computational resources. Dense representation using at most 100 million tweets.",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/davincis2023.html#ingeotec-research-group",
    "href": "IberLEF2023/davincis2023.html#ingeotec-research-group",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "INGEOTEC research group",
    "text": "INGEOTEC research group\n\n\n\n\nGitHub: https://github.com/INGEOTEC\nWebPage: https://ingeotec.github.io/",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/davincis2023.html#the-da-vincis-task-at-iberlef-2023",
    "href": "IberLEF2023/davincis2023.html#the-da-vincis-task-at-iberlef-2023",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "The DA-VINCIS Task at IberLEF 2023",
    "text": "The DA-VINCIS Task at IberLEF 2023\n\n\nAn open challenge to develop multimodal models to detect violent incidents on Twitter. The task has two tracks:\n\nviolent event identification and\nviolent event category recognition.\n\nThis presentation describes our solution system for the (i) track using only text-based features. More particularly, we used our EvoMSA framework (Graff et¬†al. 2020).",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/davincis2023.html#our-approach-evomsa-2.0",
    "href": "IberLEF2023/davincis2023.html#our-approach-evomsa-2.0",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Our approach: EvoMSA 2.0",
    "text": "Our approach: EvoMSA 2.0",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/davincis2023.html#our-representations",
    "href": "IberLEF2023/davincis2023.html#our-representations",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Our representations",
    "text": "Our representations\n\n\nSparse bag of words (SBOW)\nDense bag of words (DBOW)",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/davincis2023.html#sparse-bag-of-words",
    "href": "IberLEF2023/davincis2023.html#sparse-bag-of-words",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Sparse bag of words",
    "text": "Sparse bag of words\n\nThe text is preprocessed and tokenized, then each token \\(t\\) is associated with a vector \\(\\mathbf{v_t} \\in \\mathbb R^d\\) where the \\(i\\)-th component, i.e., \\(\\mathbf{v_t}_i\\), contains the Inverse-Document-Frequency (IDF) value of the token \\(t\\) and \\(\\forall_{j \\neq i} \\mathbf{v_t}_j=0\\).\n\nThe set of vectors \\(\\mathbf V = \\{ \\mathbf v_t \\}\\) corresponds to the vocabulary, and there are \\(|\\mathbf V| = d\\) different tokens in the vocabulary.",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/davincis2023.html#section",
    "href": "IberLEF2023/davincis2023.html#section",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "",
    "text": "A text is represented by the sequence of its tokens, i.e., \\((t_1, t_2, \\ldots)\\). The text is then vectorized as:\n\\[\n    \\textsf{sbow}(\\text{some text}) = \\textsf{sbow}((t_1, t_2, \\ldots)) = \\frac{\\sum_t \\mathbf{v_t}}{\\lVert \\sum_t \\mathbf{v_t} \\rVert}\n\\]",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/davincis2023.html#sbow",
    "href": "IberLEF2023/davincis2023.html#sbow",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "SBoW",
    "text": "SBoW",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/davincis2023.html#dense-bag-of-words",
    "href": "IberLEF2023/davincis2023.html#dense-bag-of-words",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Dense bag of words",
    "text": "Dense bag of words\n\n\nIn contrast to SBOW, the dense embeddings come from associating each component to the decision value of a text classifier (e.g., based on SBOW) pre-trained on a different collection of tweets. \nWithout loss of generality, it is assumed that there are \\(M\\) labeled datasets, each one contains a binary text classification problem.\n\n\nnoting that if a dataset has \\(K\\) labels, then this dataset can be represented as \\(K\\) binary classification problems following the one versus the rest approach, i.e., it is transformed to \\(K\\) datasets.",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/davincis2023.html#construction-of-the-dbow",
    "href": "IberLEF2023/davincis2023.html#construction-of-the-dbow",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Construction of the DBOW",
    "text": "Construction of the DBOW\n\n\nFor each of these \\(M\\) binary text classification problems, a SBOW-based classifier is built using a pre-trained SBOW representation and a linear Support Vector Machine (SVM) as the classifier. Consequently, there are \\(M\\) binary text classifiers, i.e., \\((c_1, c_2, \\ldots, c_M)\\). Additionally, the decision function of \\(c_i\\) is a value where the sign indicates the class.   The text representation is the vector obtained by concatenating the decision functions of the \\(M\\) classifiers and then normalizing the vector to have unitary norm.",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/davincis2023.html#construction-continued",
    "href": "IberLEF2023/davincis2023.html#construction-continued",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Construction (continued‚Ä¶)",
    "text": "Construction (continued‚Ä¶)\n\n\nA text \\(x\\) is represented with vector \\(\\mathbf{x^{'}} \\in \\mathbb R^M\\) where the value \\(\\mathbf{x^{'}}_i\\) corresponds to the decision function of \\(c_i\\). Given that the classifier \\(c_i\\) is a linear SVM, the decision function corresponds to the dot product between the input vector and the weight vector \\(\\mathbf w_i\\) plus the bias \\(\\mathbf w_{i_0}\\), where the weight vector and the bias are the parameters of the classifier.",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/davincis2023.html#construction-continued-1",
    "href": "IberLEF2023/davincis2023.html#construction-continued-1",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Construction (continued‚Ä¶)",
    "text": "Construction (continued‚Ä¶)\nThat is, the value \\(\\mathbf{x^{'}}_i\\) corresponds to\n\\[\n    \\mathbf{x^{'}}_i = \\mathbf w_i \\cdot \\frac{\\sum_t \\mathbf{v_t}}{\\lVert \\sum_k \\mathbf{v_k} \\rVert} + \\mathbf w_{i_0},    \n\\]\nwhere \\(\\mathbf{v_t}\\) is the IDF vector associated to the token \\(t\\) of the text \\(x\\)",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/davincis2023.html#construction-continued-2",
    "href": "IberLEF2023/davincis2023.html#construction-continued-2",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Construction (continued‚Ä¶)",
    "text": "Construction (continued‚Ä¶)\nIn matrix notation, vector \\(\\mathbf{x'}\\) is\n\\[    \\mathbf{x^{'}} = \\mathbf W \\cdot \\frac{\\sum_t \\mathbf{v_t}}{\\lVert \\sum_k \\mathbf{v_k} \\rVert} + \\mathbf{w_0},\n\\]\nwhere matrix \\(\\mathbf W \\in \\mathbb R^{M \\times d}\\) contains the weights, and \\(\\mathbf{w_0} \\in \\mathbb R^M\\) is the bias. Another way to see the previous formulation is by defining a vector \\(\\mathbf{u_t} = \\frac{1}{\\lVert \\mathbf{v_t} \\rVert} \\mathbf W \\mathbf{v_t}\\).",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/davincis2023.html#construction-continued-3",
    "href": "IberLEF2023/davincis2023.html#construction-continued-3",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Construction (continued‚Ä¶)",
    "text": "Construction (continued‚Ä¶)\nConsequently, \\(\\mathbf{x'}\\) is defined as:\n\\[\n    \\label{eq:denseBoW}\n    \\mathbf{x'} = \\sum_t \\mathbf{u_t} + \\mathbf{w_0},\n\\] vectors \\(\\mathbf{u} \\in \\mathbb R^M\\) correspond to the tokens. This is the reason we refer to this model as a dense BoW.\nFinally, the vector representing the text \\(x\\) is the normalized \\(\\mathbf{x^{'}}\\), i.e., \\(\\mathbf x = \\frac{\\mathbf{x^{'}}}{\\lVert \\mathbf{x^{'}} \\rVert}.\\)",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/davincis2023.html#dense-bow-stacking",
    "href": "IberLEF2023/davincis2023.html#dense-bow-stacking",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Dense BoW (stacking)",
    "text": "Dense BoW (stacking)\n\n\n\n\nStacking: All models aggregated are used according to their weights for producing an output, the final classification.",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/davincis2023.html#sbow-parameters-continued",
    "href": "IberLEF2023/davincis2023.html#sbow-parameters-continued",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "SBOW parameters (continued‚Ä¶)",
    "text": "SBOW parameters (continued‚Ä¶)\n\nThe pre-trained BoW is estimated from 4,194,304 (\\(2^{22}\\)) tweets randomly selected in a larger collection of messages.\nThe IDF values were estimated from the collections, and some tokens were selected from all the available ones found in the collection.",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/davincis2023.html#sbow-parameters-continued-1",
    "href": "IberLEF2023/davincis2023.html#sbow-parameters-continued-1",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "SBOW parameters (continued‚Ä¶)",
    "text": "SBOW parameters (continued‚Ä¶)\nTwo procedures were used to select the tokens:\n\nthe first corresponds to selecting the \\(d\\) tokens with the highest frequency, and the other to normalize the frequency w.r.t. their type, i.e., bigrams, words, and q-grams of characters. Once the frequency is normalized, one selects the \\(d\\) tokens with the highest normalized frequency.\nThe value of \\(d\\) is \\(2^{17}\\); however, one can also find in the library models for \\(2^{13}, 2^{14}, \\ldots, 2^{17}.\\)",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/davincis2023.html#sbow-parameters-continued-2",
    "href": "IberLEF2023/davincis2023.html#sbow-parameters-continued-2",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "SBOW parameters (continued‚Ä¶)",
    "text": "SBOW parameters (continued‚Ä¶)\n\nIt is also possible to train the BoW model using the training set; in this case, we used the default parameters. The only difference is that vocabulary size \\(d\\) is bounded by the training set tokens.",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/davincis2023.html#dbow-parameters-continued",
    "href": "IberLEF2023/davincis2023.html#dbow-parameters-continued",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "DBOW parameters (continued‚Ä¶)",
    "text": "DBOW parameters (continued‚Ä¶)\nFollowing an equivalent approach used in the development of the pre-trained BoW, different dense representations were created.\nThese correspond to varying the size of the vocabulary and the two procedures used to select the tokens. Vector spaces:\n\ndataset is in \\(\\mathbb R^{57}\\).\nemoji is in \\(\\mathbb R^{567}\\).\nkeyword is in \\(\\mathbb R^{2048}\\).",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/davincis2023.html#configurations-continued",
    "href": "IberLEF2023/davincis2023.html#configurations-continued",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Configurations (continued‚Ä¶)",
    "text": "Configurations (continued‚Ä¶)\nThe different configurations tested in this competition are described below. These configurations include BoW and a combination of BoW with dense representations. Stack generalization combines the different text classifiers, and the top classifier was a Naive Bayes algorithm. The specific implementation of this configuration can be seen in EvoMSA‚Äôs documentation.",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/davincis2023.html#configurations-continued-1",
    "href": "IberLEF2023/davincis2023.html#configurations-continued-1",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Configurations (continued‚Ä¶)",
    "text": "Configurations (continued‚Ä¶)\nThe list of configurations\n\n\nbow: Pre-trained BoW where the tokens are selected based on a normalized frequency w.r.t. its type, i.e., bigrams, words, and q-grams of characters.\nbow_voc_selection: Pre-trained BoW where the tokens correspond to the most frequent ones.\nbow_training_set: BoW trained with the training set; the number of tokens corresponds to all the tokens in the set.\nstack_bow_keywords_emojis: Stack generalization approach where the base classifiers are the BoW, the emojis, and the keywords dense BoW.\n\nstack_bow_keywords_emojis_voc_selection: Stack generalization approach where the base classifiers are the BoW, the emojis, and the keywords dense BoW. The tokens in these models were selected based on a normalized frequency w.r.t. its type, i.e., bigrams, words, and q-grams of characters.",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/davincis2023.html#configurations-continued-2",
    "href": "IberLEF2023/davincis2023.html#configurations-continued-2",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Configurations (continued‚Ä¶)",
    "text": "Configurations (continued‚Ä¶)\n\n\nstack_bows: Stack generalization approach where the base classifiers are BoW with the two token selection procedures described previously (i.e., bow and bow_voc_selection).\nstack_2_bow_keywords: Stack generalization approach where with four base classifiers. These correspond to two BoW and two dense BoW (emojis and keywords), where the difference in each is the procedure used to select the tokens, i.e., the most frequent or normalized frequency.\nstack_2_bow_tailored_keywords: Stack generalization approach with four base classifiers. These correspond to two BoW and two dense BoW (emojis and keywords), where the difference in each is the procedure used to select the tokens, i.e., the most frequent or normalized frequency. The second difference is that the dense representation with normalized frequency also includes models for the most discriminant words selected by a BoW classifier in the training set. We refer to these latter representations as tailored keywords.",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/davincis2023.html#configurations-continued-3",
    "href": "IberLEF2023/davincis2023.html#configurations-continued-3",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Configurations (continued‚Ä¶)",
    "text": "Configurations (continued‚Ä¶)\n\n\nstack_2_bow_all_keywords: Stack generalization approach with four base classifiers equivalent to stack_2_bow_keywords where the difference is that the dense representations include the models created with the human-annotated datasets.\nstack_2_bow_tailored_all_keywords: Stack generalization approach with four base classifiers equivalent to stack_2_bow_all_keywords, where the difference is that the dense representation with normalized frequency also includes the tailored keywords.\nstack_3_bows: Stack generalization approach with three base classifiers. All of them are BoW; the first two correspond pre-trained BoW with the two token selection procedures described previously (i.e., bow and bow_voc_selection), and the latest is a BoW trained on the training set (i.e., bow_training_set).",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/davincis2023.html#configurations-continued-4",
    "href": "IberLEF2023/davincis2023.html#configurations-continued-4",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Configurations (continued‚Ä¶)",
    "text": "Configurations (continued‚Ä¶)\n\n\nstack_3_bows_tailored_keywords: Stack generalization approach with five base classifiers. The first corresponds to a BoW trained on the training set, and the rest are used in stack_2_bow_tailored_keywords.\nstack_3_bow_tailored_all_keywords: Stack generalization approach with five base classifiers. It is comparable to stack_3_bows_tailored_keywords being the difference in the use of the tailored keywords.",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/davincis2023.html#competition",
    "href": "IberLEF2023/davincis2023.html#competition",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Competition",
    "text": "Competition\n\n\n\nPerformance comparison of our submission (INGEOTEC) and the competition‚Äôs winner in each edition. The best performance is in boldface.\n\n\n\nDA-VINCIS 2023\nDA-VINCIS 20222\n\n\n\n\nWinner\n0.9264\n0.7817\n\n\nINGEOTEC\n0.8903\n0.7510\n\n\nDifference\n4.1%\n4.1%\n\n\n\n\n\nAnother comparison that one can make with the Table‚Äôs data is computing the difference between the best performance in k-fold cross-validation and the worst; it can be observed that for the 2023 edition, the difference is 1.4%, and for 2022 is 13.3%. The difference of 1.4% indicates that following this approach will be complicated to improve. On the other hand, for the edition 2022, there might be room for improvement following the presented approach.",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/davincis2023.html#other-competitions-performance",
    "href": "IberLEF2023/davincis2023.html#other-competitions-performance",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Other competitions performance",
    "text": "Other competitions performance\n\nPerformance comparison. EvoMSA‚Äô results\n\n\nCompetition\nWinner\nEvoMSA 2.0\nDifference\n\n\n\n\nPoliticEs (Gender)\n0.8296\n0.7115\n16.6%\n\n\nPoliticEs (Profession)\n0.8608\n0.8379\n2.7%\n\n\nPoliticEs (Ideology Binary)\n0.8967\n0.8913\n0.6%\n\n\nPoliticEs (Ideology Multiclass)\n0.6913\n0.6694\n3.3%\n\n\nREST-MEX (Polarity)\n0.6216\n0.5548\n12.0%\n\n\nREST-MEX (Type)\n0.9903\n0.9805\n1.0%\n\n\nREST-MEX (Country)\n0.9420\n0.9270\n1.6%",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/davincis2023.html#other-competitions-performance-continued",
    "href": "IberLEF2023/davincis2023.html#other-competitions-performance-continued",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Other competitions performance (Continued‚Ä¶)",
    "text": "Other competitions performance (Continued‚Ä¶)\n\nPerformance comparison. EvoMSA‚Äô results\n\n\nCompetition\nWinner\nEvoMSA 2.0\nDifference\n\n\n\n\nHOMO-MEX\n0.8847\n0.8050\n9.9%\n\n\nHOPE (ES)\n0.9161\n0.4198\n118.2%\n\n\nHOPE (EN)\n0.5012\n0.4429\n13.2%\n\n\nDIPROMATS (ES)\n0.8089\n0.7485\n8.1%\n\n\nDIPROMATS (EN)\n0.8090\n0.7255\n11.5%\n\n\nHUHU\n0.820\n0.775\n5.8%",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/davincis2023.html#example",
    "href": "IberLEF2023/davincis2023.html#example",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Example",
    "text": "Example\n\nUsing a tailored dense model with the DA-VINCIS 2023 dataset, we can see:",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/davincis2023.html#example---performance-varying-d",
    "href": "IberLEF2023/davincis2023.html#example---performance-varying-d",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Example - performance varying \\(d\\)",
    "text": "Example - performance varying \\(d\\)",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/davincis2023.html#conclusions-continued",
    "href": "IberLEF2023/davincis2023.html#conclusions-continued",
    "title": "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers",
    "section": "Conclusions (continued‚Ä¶)",
    "text": "Conclusions (continued‚Ä¶)\n\n\nExplainability of the model (with a simple bow outstanding results). Simplest solution Fast solution (in training and test), low computational resources. Dense representation using at most 100 million tweets.",
    "crumbs": [
      "Ingeotec at DA-VINCIS: Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/RestMex2023-10mins.html#ingeotec-research-group",
    "href": "IberLEF2023/RestMex2023-10mins.html#ingeotec-research-group",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "INGEOTEC research group",
    "text": "INGEOTEC research group\n\n\n\n\nGitHub: https://github.com/INGEOTEC\nWebPage: https://ingeotec.github.io/",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/RestMex2023-10mins.html#our-approach-evomsa-2.0",
    "href": "IberLEF2023/RestMex2023-10mins.html#our-approach-evomsa-2.0",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "Our approach: EvoMSA 2.0",
    "text": "Our approach: EvoMSA 2.0\n \n\n\n\nEvoMSA‚Äôs documentation (https://evomsa.readthedocs) Papers (Graff et¬†al. 2020) (Tellez et¬†al. 2017)",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/RestMex2023-10mins.html#our-representations",
    "href": "IberLEF2023/RestMex2023-10mins.html#our-representations",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "Our representations",
    "text": "Our representations\n\n\nSparse bag of words (SBOW)\nDense bag of words (DBOW)",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/RestMex2023-10mins.html#sbow",
    "href": "IberLEF2023/RestMex2023-10mins.html#sbow",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "SBoW",
    "text": "SBoW",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/RestMex2023-10mins.html#dense-bow-stacking",
    "href": "IberLEF2023/RestMex2023-10mins.html#dense-bow-stacking",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "Dense BoW (stacking)",
    "text": "Dense BoW (stacking)\n\n\n\n\nStacking: All models aggregated are used according to their weights for producing an output, the final classification.",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/RestMex2023-10mins.html#dbow-parameters",
    "href": "IberLEF2023/RestMex2023-10mins.html#dbow-parameters",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "DBOW parameters",
    "text": "DBOW parameters\nFollowing an equivalent approach used in the development of the pre-trained BoW, different dense representations were created.\nThese correspond to varying the size of the vocabulary and the two procedures used to select the tokens. Vector spaces:\n\ndataset is in \\(\\mathbb R^{57}\\).\nemoji is in \\(\\mathbb R^{567}\\).\nkeyword is in \\(\\mathbb R^{2048}\\).",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/RestMex2023-10mins.html#configurations-1",
    "href": "IberLEF2023/RestMex2023-10mins.html#configurations-1",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "Configurations",
    "text": "Configurations\nThe different configurations tested in this competition are described below. These configurations include BoW and a combination of BoW with dense representations. Stack generalization combines the different text classifiers, and the top classifier was a Naive Bayes algorithm. The specific implementation of this configuration can be seen in EvoMSA‚Äôs documentation.",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/RestMex2023-10mins.html#competition",
    "href": "IberLEF2023/RestMex2023-10mins.html#competition",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "Competition",
    "text": "Competition\n\n\n\nPerformance comparison of our submission (INGEOTEC) and the competition‚Äôs winner. The best performance is in boldface.\n\n\n\nType\nCountry\nPolarity\n\n\n\n\nWinner\n0.9903\n0.9420\n0.6217\n\n\nINGEOTEC\n0.9805\n0.9271\n0.5549\n\n\nDifference\n1%\n1.6%\n12.0%\n\n\n\n\n\nAnother comparison that one can make with the Table‚Äôs data is computing the difference between the best performance in k-fold cross-validation and the worst; it can be observed that for the 2023 edition, the difference is 1.4%, and for 2022 is 13.3%. The difference of 1.4% indicates that following this approach will be complicated to improve. On the other hand, for the edition 2022, there might be room for improvement following the presented approach.",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/RestMex2023-10mins.html#other-competitions-performance",
    "href": "IberLEF2023/RestMex2023-10mins.html#other-competitions-performance",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "Other competitions performance",
    "text": "Other competitions performance\n\nPerformance comparison. EvoMSA‚Äô results\n\n\nCompetition\nWinner\nEvoMSA 2.0\nDifference\n\n\n\n\nPoliticEs (Gender)\n0.8296\n0.7115\n16.6%\n\n\nPoliticEs (Profession)\n0.8608\n0.8379\n2.7%\n\n\nPoliticEs (Ideology Binary)\n0.8967\n0.8913\n0.6%\n\n\nPoliticEs (Ideology Multiclass)\n0.6913\n0.6694\n3.3%\n\n\nREST-MEX (Polarity)\n0.6216\n0.5548\n12.0%\n\n\nREST-MEX (Type)\n0.9903\n0.9805\n1.0%\n\n\nREST-MEX (Country)\n0.9420\n0.9270\n1.6%",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/RestMex2023-10mins.html#other-competitions-performance-continued",
    "href": "IberLEF2023/RestMex2023-10mins.html#other-competitions-performance-continued",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "Other competitions performance (Continued‚Ä¶)",
    "text": "Other competitions performance (Continued‚Ä¶)\n\nPerformance comparison. EvoMSA‚Äô results\n\n\nCompetition\nWinner\nEvoMSA 2.0\nDifference\n\n\n\n\nHOMO-MEX\n0.8847\n0.8050\n9.9%\n\n\nHOPE (ES)\n0.9161\n0.4198\n118.2%\n\n\nHOPE (EN)\n0.5012\n0.4429\n13.2%\n\n\nDIPROMATS (ES)\n0.8089\n0.7485\n8.1%\n\n\nDIPROMATS (EN)\n0.8090\n0.7255\n11.5%\n\n\nHUHU\n0.820\n0.775\n5.8%",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/RestMex2023-10mins.html#conclusions-continued",
    "href": "IberLEF2023/RestMex2023-10mins.html#conclusions-continued",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "Conclusions (continued‚Ä¶)",
    "text": "Conclusions (continued‚Ä¶)\n\n\n\nOur results show that developing competitive models for violent event identification is possible using only text-based features and, even more, bag-of-words-based models.\nExplainability of the model (with a simple bow outstanding results). Simplest solution\nFast solution (in training and test), low computational resources. Dense representation using at most 100 million tweets.",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/restMex2023.html#ingeotec-research-group",
    "href": "IberLEF2023/restMex2023.html#ingeotec-research-group",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "INGEOTEC research group",
    "text": "INGEOTEC research group\n\n\n\n\nGitHub: https://github.com/INGEOTEC\nWebPage: https://ingeotec.github.io/",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/restMex2023.html#the-rest-mex-task-at-iberlef-2023",
    "href": "IberLEF2023/restMex2023.html#the-rest-mex-task-at-iberlef-2023",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "The Rest-Mex Task at IberLEF 2023",
    "text": "The Rest-Mex Task at IberLEF 2023\n\n\nFor the sentiment analysis, the problem is resume as:\n\n\n‚ÄúGiven an opinion about a Mexican tourist place, the goal is to determine the polarity, between 1 and 5, of the text, the type of opinion (hotel, restaurant or attraction) and, the country of the place of which the opinion is being given (Mexico, Cuba, Colombia)‚Äù",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/restMex2023.html#our-appproach",
    "href": "IberLEF2023/restMex2023.html#our-appproach",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "Our appproach",
    "text": "Our appproach\n Our solution system uses only text-based features. More particularly, we used our EvoMSA framework (Graff et¬†al. 2020).",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/restMex2023.html#our-approach-evomsa-2.0",
    "href": "IberLEF2023/restMex2023.html#our-approach-evomsa-2.0",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "Our approach: EvoMSA 2.0",
    "text": "Our approach: EvoMSA 2.0",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/restMex2023.html#our-representations",
    "href": "IberLEF2023/restMex2023.html#our-representations",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "Our representations",
    "text": "Our representations\n\n\nSparse bag of words (SBOW)\nDense bag of words (DBOW)",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/restMex2023.html#sparse-bag-of-words",
    "href": "IberLEF2023/restMex2023.html#sparse-bag-of-words",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "Sparse bag of words",
    "text": "Sparse bag of words\n\nThe text is preprocessed and tokenized, then each token \\(t\\) is associated with a vector \\(\\mathbf{v_t} \\in \\mathbb R^d\\) where the \\(i\\)-th component, i.e., \\(\\mathbf{v_t}_i\\), contains the Inverse-Document-Frequency (IDF) value of the token \\(t\\) and \\(\\forall_{j \\neq i} \\mathbf{v_t}_j=0\\).\n\nThe set of vectors \\(\\mathbf V = \\{ \\mathbf v_t \\}\\) corresponds to the vocabulary, and there are \\(|\\mathbf V| = d\\) different tokens in the vocabulary.",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/restMex2023.html#section",
    "href": "IberLEF2023/restMex2023.html#section",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "",
    "text": "A text is represented by the sequence of its tokens, i.e., \\((t_1, t_2, \\ldots)\\). The text is then vectorized as:\n\\[\n    \\textsf{sbow}(\\text{some text}) = \\textsf{sbow}((t_1, t_2, \\ldots)) = \\frac{\\sum_t \\mathbf{v_t}}{\\lVert \\sum_t \\mathbf{v_t} \\rVert}\n\\]",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/restMex2023.html#sbow",
    "href": "IberLEF2023/restMex2023.html#sbow",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "SBoW",
    "text": "SBoW",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/restMex2023.html#dense-bag-of-words",
    "href": "IberLEF2023/restMex2023.html#dense-bag-of-words",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "Dense bag of words",
    "text": "Dense bag of words\n\n\nIn contrast to SBOW, the dense embeddings come from associating each component to the decision value of a text classifier (e.g., based on SBOW) pre-trained on a different collection of tweets. \nWithout loss of generality, it is assumed that there are \\(M\\) labeled datasets, each one contains a binary text classification problem.\n\n\nnoting that if a dataset has \\(K\\) labels, then this dataset can be represented as \\(K\\) binary classification problems following the one versus the rest approach, i.e., it is transformed to \\(K\\) datasets.",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/restMex2023.html#construction-of-the-dbow",
    "href": "IberLEF2023/restMex2023.html#construction-of-the-dbow",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "Construction of the DBOW",
    "text": "Construction of the DBOW\n\n\nFor each of these \\(M\\) binary text classification problems, a SBOW-based classifier is built using a pre-trained SBOW representation and a linear Support Vector Machine (SVM) as the classifier. Consequently, there are \\(M\\) binary text classifiers, i.e., \\((c_1, c_2, \\ldots, c_M)\\). Additionally, the decision function of \\(c_i\\) is a value where the sign indicates the class.   The text representation is the vector obtained by concatenating the decision functions of the \\(M\\) classifiers and then normalizing the vector to have unitary norm.",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/restMex2023.html#construction-continued",
    "href": "IberLEF2023/restMex2023.html#construction-continued",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "Construction (continued‚Ä¶)",
    "text": "Construction (continued‚Ä¶)\n\n\nA text \\(x\\) is represented with vector \\(\\mathbf{x^{'}} \\in \\mathbb R^M\\) where the value \\(\\mathbf{x^{'}}_i\\) corresponds to the decision function of \\(c_i\\). Given that the classifier \\(c_i\\) is a linear SVM, the decision function corresponds to the dot product between the input vector and the weight vector \\(\\mathbf w_i\\) plus the bias \\(\\mathbf w_{i_0}\\), where the weight vector and the bias are the parameters of the classifier.",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/restMex2023.html#construction-continued-1",
    "href": "IberLEF2023/restMex2023.html#construction-continued-1",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "Construction (continued‚Ä¶)",
    "text": "Construction (continued‚Ä¶)\nThat is, the value \\(\\mathbf{x^{'}}_i\\) corresponds to\n\\[\n    \\mathbf{x^{'}}_i = \\mathbf w_i \\cdot \\frac{\\sum_t \\mathbf{v_t}}{\\lVert \\sum_k \\mathbf{v_k} \\rVert} + \\mathbf w_{i_0},    \n\\]\nwhere \\(\\mathbf{v_t}\\) is the IDF vector associated to the token \\(t\\) of the text \\(x\\)",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/restMex2023.html#construction-continued-2",
    "href": "IberLEF2023/restMex2023.html#construction-continued-2",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "Construction (continued‚Ä¶)",
    "text": "Construction (continued‚Ä¶)\nIn matrix notation, vector \\(\\mathbf{x'}\\) is\n\\[    \\mathbf{x^{'}} = \\mathbf W \\cdot \\frac{\\sum_t \\mathbf{v_t}}{\\lVert \\sum_k \\mathbf{v_k} \\rVert} + \\mathbf{w_0},\n\\]\nwhere matrix \\(\\mathbf W \\in \\mathbb R^{M \\times d}\\) contains the weights, and \\(\\mathbf{w_0} \\in \\mathbb R^M\\) is the bias. Another way to see the previous formulation is by defining a vector \\(\\mathbf{u_t} = \\frac{1}{\\lVert \\mathbf{v_t} \\rVert} \\mathbf W \\mathbf{v_t}\\).",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/restMex2023.html#construction-continued-3",
    "href": "IberLEF2023/restMex2023.html#construction-continued-3",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "Construction (continued‚Ä¶)",
    "text": "Construction (continued‚Ä¶)\nConsequently, \\(\\mathbf{x'}\\) is defined as:\n\\[\n    \\label{eq:denseBoW}\n    \\mathbf{x'} = \\sum_t \\mathbf{u_t} + \\mathbf{w_0},\n\\] vectors \\(\\mathbf{u} \\in \\mathbb R^M\\) correspond to the tokens. This is the reason we refer to this model as a dense BoW.\nFinally, the vector representing the text \\(x\\) is the normalized \\(\\mathbf{x^{'}}\\), i.e., \\(\\mathbf x = \\frac{\\mathbf{x^{'}}}{\\lVert \\mathbf{x^{'}} \\rVert}.\\)",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/restMex2023.html#dense-bow-stacking",
    "href": "IberLEF2023/restMex2023.html#dense-bow-stacking",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "Dense BoW (stacking)",
    "text": "Dense BoW (stacking)\n\n\n\n\nStacking: All models aggregated are used according to their weights for producing an output, the final classification.",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/restMex2023.html#sbow-parameters-continued",
    "href": "IberLEF2023/restMex2023.html#sbow-parameters-continued",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "SBOW parameters (continued‚Ä¶)",
    "text": "SBOW parameters (continued‚Ä¶)\n\nThe pre-trained BoW is estimated from 4,194,304 (\\(2^{22}\\)) tweets randomly selected in a larger collection of messages.\nThe IDF values were estimated from the collections, and some tokens were selected from all the available ones found in the collection.",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/restMex2023.html#sbow-parameters-continued-1",
    "href": "IberLEF2023/restMex2023.html#sbow-parameters-continued-1",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "SBOW parameters (continued‚Ä¶)",
    "text": "SBOW parameters (continued‚Ä¶)\nTwo procedures were used to select the tokens:\n\nthe first corresponds to selecting the \\(d\\) tokens with the highest frequency, and the other to normalize the frequency w.r.t. their type, i.e., bigrams, words, and q-grams of characters. Once the frequency is normalized, one selects the \\(d\\) tokens with the highest normalized frequency.\nThe value of \\(d\\) is \\(2^{17}\\); however, one can also find in the library models for \\(2^{13}, 2^{14}, \\ldots, 2^{17}.\\)",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/restMex2023.html#sbow-parameters-continued-2",
    "href": "IberLEF2023/restMex2023.html#sbow-parameters-continued-2",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "SBOW parameters (continued‚Ä¶)",
    "text": "SBOW parameters (continued‚Ä¶)\n\nIt is also possible to train the BoW model using the training set; in this case, we used the default parameters. The only difference is that vocabulary size \\(d\\) is bounded by the training set tokens.",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/restMex2023.html#dbow-parameters-continued",
    "href": "IberLEF2023/restMex2023.html#dbow-parameters-continued",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "DBOW parameters (continued‚Ä¶)",
    "text": "DBOW parameters (continued‚Ä¶)\nFollowing an equivalent approach used in the development of the pre-trained BoW, different dense representations were created.\nThese correspond to varying the size of the vocabulary and the two procedures used to select the tokens. Vector spaces:\n\ndataset is in \\(\\mathbb R^{57}\\).\nemoji is in \\(\\mathbb R^{567}\\).\nkeyword is in \\(\\mathbb R^{2048}\\).",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/restMex2023.html#configurations-continued",
    "href": "IberLEF2023/restMex2023.html#configurations-continued",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "Configurations (continued‚Ä¶)",
    "text": "Configurations (continued‚Ä¶)\nThe different configurations tested in this competition are described below. These configurations include BoW and a combination of BoW with dense representations. Stack generalization combines the different text classifiers, and the top classifier was a Naive Bayes algorithm. The specific implementation of this configuration can be seen in EvoMSA‚Äôs documentation.",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/restMex2023.html#configurations-continued-1",
    "href": "IberLEF2023/restMex2023.html#configurations-continued-1",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "Configurations (continued‚Ä¶)",
    "text": "Configurations (continued‚Ä¶)\nThe list of configurations\n\n\nbow: Pre-trained BoW where the tokens are selected based on a normalized frequency w.r.t. its type, i.e., bigrams, words, and q-grams of characters.\nbow_voc_selection: Pre-trained BoW where the tokens correspond to the most frequent ones.\nbow_training_set: BoW trained with the training set; the number of tokens corresponds to all the tokens in the set.\nstack_bow_keywords_emojis: Stack generalization approach where the base classifiers are the BoW, the emojis, and the keywords dense BoW.\n\nstack_bow_keywords_emojis_voc_selection: Stack generalization approach where the base classifiers are the BoW, the emojis, and the keywords dense BoW. The tokens in these models were selected based on a normalized frequency w.r.t. its type, i.e., bigrams, words, and q-grams of characters.",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/restMex2023.html#configurations-continued-2",
    "href": "IberLEF2023/restMex2023.html#configurations-continued-2",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "Configurations (continued‚Ä¶)",
    "text": "Configurations (continued‚Ä¶)\n\n\nstack_bows: Stack generalization approach where the base classifiers are BoW with the two token selection procedures described previously (i.e., bow and bow_voc_selection).\nstack_2_bow_keywords: Stack generalization approach where with four base classifiers. These correspond to two BoW and two dense BoW (emojis and keywords), where the difference in each is the procedure used to select the tokens, i.e., the most frequent or normalized frequency.\nstack_2_bow_tailored_keywords: Stack generalization approach with four base classifiers. These correspond to two BoW and two dense BoW (emojis and keywords), where the difference in each is the procedure used to select the tokens, i.e., the most frequent or normalized frequency. The second difference is that the dense representation with normalized frequency also includes models for the most discriminant words selected by a BoW classifier in the training set. We refer to these latter representations as tailored keywords.",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/restMex2023.html#configurations-continued-3",
    "href": "IberLEF2023/restMex2023.html#configurations-continued-3",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "Configurations (continued‚Ä¶)",
    "text": "Configurations (continued‚Ä¶)\n\n\nstack_2_bow_all_keywords: Stack generalization approach with four base classifiers equivalent to stack_2_bow_keywords where the difference is that the dense representations include the models created with the human-annotated datasets.\nstack_2_bow_tailored_all_keywords: Stack generalization approach with four base classifiers equivalent to stack_2_bow_all_keywords, where the difference is that the dense representation with normalized frequency also includes the tailored keywords.\nstack_3_bows: Stack generalization approach with three base classifiers. All of them are BoW; the first two correspond pre-trained BoW with the two token selection procedures described previously (i.e., bow and bow_voc_selection), and the latest is a BoW trained on the training set (i.e., bow_training_set).",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/restMex2023.html#configurations-continued-4",
    "href": "IberLEF2023/restMex2023.html#configurations-continued-4",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "Configurations (continued‚Ä¶)",
    "text": "Configurations (continued‚Ä¶)\n\n\nstack_3_bows_tailored_keywords: Stack generalization approach with five base classifiers. The first corresponds to a BoW trained on the training set, and the rest are used in stack_2_bow_tailored_keywords.\nstack_3_bow_tailored_all_keywords: Stack generalization approach with five base classifiers. It is comparable to stack_3_bows_tailored_keywords being the difference in the use of the tailored keywords.",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/restMex2023.html#competition",
    "href": "IberLEF2023/restMex2023.html#competition",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "Competition",
    "text": "Competition\n\n\n\nPerformance comparison of our submission (INGEOTEC) and the competition‚Äôs winner. The best performance is in boldface.\n\n\n\nType\nCountry\nPolarity\n\n\n\n\nWinner\n0.9903\n0.9420\n0.6217\n\n\nINGEOTEC\n0.9805\n0.9271\n0.5549\n\n\nDifference\n1%\n1.6%\n12.0%\n\n\n\n\n\nAnother comparison that one can make with the Table‚Äôs data is computing the difference between the best performance in k-fold cross-validation and the worst; it can be observed that for the 2023 edition, the difference is 1.4%, and for 2022 is 13.3%. The difference of 1.4% indicates that following this approach will be complicated to improve. On the other hand, for the edition 2022, there might be room for improvement following the presented approach.",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/restMex2023.html#other-competitions-performance",
    "href": "IberLEF2023/restMex2023.html#other-competitions-performance",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "Other competitions performance",
    "text": "Other competitions performance\n\nPerformance comparison. EvoMSA‚Äô results\n\n\nCompetition\nWinner\nEvoMSA 2.0\nDifference\n\n\n\n\nPoliticEs (Gender)\n0.8296\n0.7115\n16.6%\n\n\nPoliticEs (Profession)\n0.8608\n0.8379\n2.7%\n\n\nPoliticEs (Ideology Binary)\n0.8967\n0.8913\n0.6%\n\n\nPoliticEs (Ideology Multiclass)\n0.6913\n0.6694\n3.3%\n\n\nREST-MEX (Polarity)\n0.6216\n0.5548\n12.0%\n\n\nREST-MEX (Type)\n0.9903\n0.9805\n1.0%\n\n\nREST-MEX (Country)\n0.9420\n0.9270\n1.6%",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/restMex2023.html#other-competitions-performance-continued",
    "href": "IberLEF2023/restMex2023.html#other-competitions-performance-continued",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "Other competitions performance (Continued‚Ä¶)",
    "text": "Other competitions performance (Continued‚Ä¶)\n\nPerformance comparison. EvoMSA‚Äô results\n\n\nCompetition\nWinner\nEvoMSA 2.0\nDifference\n\n\n\n\nHOMO-MEX\n0.8847\n0.8050\n9.9%\n\n\nHOPE (ES)\n0.9161\n0.4198\n118.2%\n\n\nHOPE (EN)\n0.5012\n0.4429\n13.2%\n\n\nDIPROMATS (ES)\n0.8089\n0.7485\n8.1%\n\n\nDIPROMATS (EN)\n0.8090\n0.7255\n11.5%\n\n\nHUHU\n0.820\n0.775\n5.8%",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "IberLEF2023/restMex2023.html#conclusions-continued",
    "href": "IberLEF2023/restMex2023.html#conclusions-continued",
    "title": "Ingeotec at Rest-Mex: ¬†Bag-of-Words Classifiers",
    "section": "Conclusions (continued‚Ä¶)",
    "text": "Conclusions (continued‚Ä¶)\n\n\n\nOur results show that developing competitive models for violent event identification is possible using only text-based features and, even more, bag-of-words-based models.\nExplainability of the model (with a simple bow outstanding results). Simplest solution\nFast solution (in training and test), low computational resources. Dense representation using at most 100 million tweets.",
    "crumbs": [
      "Ingeotec at Rest-Mex: \\ Bag-of-Words Classifiers"
    ]
  },
  {
    "objectID": "TextClassification/general.html#ingeotec",
    "href": "TextClassification/general.html#ingeotec",
    "title": "Text Classification",
    "section": "INGEOTEC",
    "text": "INGEOTEC\n\n\nGitHub: https://github.com/INGEOTEC\nWebPage: https://ingeotec.github.io/",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#aguascalientes-m√©xico",
    "href": "TextClassification/general.html#aguascalientes-m√©xico",
    "title": "Text Classification",
    "section": "Aguascalientes, M√©xico",
    "text": "Aguascalientes, M√©xico",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#opinion-mining-1",
    "href": "TextClassification/general.html#opinion-mining-1",
    "title": "Text Classification",
    "section": "Opinion Mining",
    "text": "Opinion Mining\n\n\n\nDefinition\n\n\nStudy people‚Äôs opinions, appraisals, attitudes, and emotions toward entities, individuals, events, and their attributes.\n\n\n\n\nDistilling opinions from texts\nA brand is interested in the costumer‚Äôs opinions\nHuge amount of information",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#opinion-mining-2",
    "href": "TextClassification/general.html#opinion-mining-2",
    "title": "Text Classification",
    "section": "Opinion Mining (2)",
    "text": "Opinion Mining (2)\n\n\n\nFormal Definition\n\n\n\n\\(e_i\\) - Entity\n\\(a_{ij}\\) - Aspect of \\(e_i\\)\n\\(o_{ijkl}\\) - Opinion orientation\n\\(h_k\\) - Opinion source - Opinion holder\n\\(t_l\\) - Time of the opinion\n\n\n\n\n\n\n\nEntity\n\n\nProduct, service, person, event, organization, or topic\n\n\n\n\n\n\nAspect\n\n\nEntity‚Äôs component or attribute",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#opinion-mining-tasks",
    "href": "TextClassification/general.html#opinion-mining-tasks",
    "title": "Text Classification",
    "section": "Opinion Mining Tasks",
    "text": "Opinion Mining Tasks\n\n\n\nTasks\n\n\n\nEntity extraction\nAspect extraction - considered the entities\nIdentify opinion source and time\nIdentify opinion orientation",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#text-classification",
    "href": "TextClassification/general.html#text-classification",
    "title": "Text Classification",
    "section": "Text Classification",
    "text": "Text Classification\n\n\n\nDefinition\n\n\nThe aim is the classification of documents into a fixed number of predefined categories.\n\n\n\n\n\n\nPolarity\n\n\nEl d√≠a de ma√±ana no podr√© ir con ustedes a la librer√≠a\n\n\n\n\nNegative",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#text-classification-tasks",
    "href": "TextClassification/general.html#text-classification-tasks",
    "title": "Text Classification",
    "section": "Text Classification Tasks",
    "text": "Text Classification Tasks\n\n\n\nPolarity\n\n\nPositive, Negative, Neutral\n\n\n\n\n\n\nEmotion (Multiclass)\n\n\n\nAnger, Joy, ‚Ä¶\nIntensity of an emotion\n\n\n\n\n\n\n\nEvent (Binary)\n\n\n\nViolent\nCrime",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#profiling",
    "href": "TextClassification/general.html#profiling",
    "title": "Text Classification",
    "section": "Profiling",
    "text": "Profiling\n\n\n\nGender\n\n\nMan, Woman, Nonbinary, ‚Ä¶\n\n\n\n\n\n\nAge\n\n\nChild, Teen, Adult\n\n\n\n\n\n\nLanguage Variety\n\n\n\nSpanish: Spain, Cuba, Argentine, M√©xico, ‚Ä¶\nEnglish: United States, England, ‚Ä¶",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#machine-learning",
    "href": "TextClassification/general.html#machine-learning",
    "title": "Text Classification",
    "section": "Machine Learning",
    "text": "Machine Learning\n\n\n\nDefinition\n\n\nMachine learning (ML) is a subfield of artificial intelligence that focuses on the development and implementation of algorithms capable of learning from data without being explicitly programmed.\n\n\n\n\n\n\nTypes of ML algorithms\n\n\n\nUnsupervised Learning\nSupervised Learning\nReinforcement Learning",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#supervised-learning-multiclass",
    "href": "TextClassification/general.html#supervised-learning-multiclass",
    "title": "Text Classification",
    "section": "Supervised Learning (Multiclass)",
    "text": "Supervised Learning (Multiclass)",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#supervised-learning-binary",
    "href": "TextClassification/general.html#supervised-learning-binary",
    "title": "Text Classification",
    "section": "Supervised Learning (Binary)",
    "text": "Supervised Learning (Binary)",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#supervised-learning-classification",
    "href": "TextClassification/general.html#supervised-learning-classification",
    "title": "Text Classification",
    "section": "Supervised Learning (Classification)",
    "text": "Supervised Learning (Classification)\n\n\n                                                \n\n\n\n\n\n\nDecision function\n\n\n\n\n\\(g(\\mathbf x) = -0.78 x_1 + 0.60 x_2 + -0.88\\)",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#supervised-learning-geometry",
    "href": "TextClassification/general.html#supervised-learning-geometry",
    "title": "Text Classification",
    "section": "Supervised Learning (Geometry)",
    "text": "Supervised Learning (Geometry)\n\n\n                                                \n\n\n\n\n\nDecision function\n\n\n\n\n\\(g(\\mathbf x) = -0.78 x_1 + 0.60 x_2 + -0.88\\)",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#supervised-learning-geometry-2",
    "href": "TextClassification/general.html#supervised-learning-geometry-2",
    "title": "Text Classification",
    "section": "Supervised Learning (Geometry 2)",
    "text": "Supervised Learning (Geometry 2)\n\n\n                                                \n\n\n\n\n\n\\(w_0\\)\n\n\n\n\n\n\\(w_0 = -0.88\\)\n\\(w_0 = 0.88\\)",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#supervised-learning-geometry-3",
    "href": "TextClassification/general.html#supervised-learning-geometry-3",
    "title": "Text Classification",
    "section": "Supervised Learning (Geometry 3)",
    "text": "Supervised Learning (Geometry 3)\n\n\n                                                \n\n\n\n\n\nDecision function\n\n\n\n\n\n\\(g_{svm}(\\mathbf x) = -0.78 x_1 + 0.60 x_2 + -0.88\\)\n\\(g_{lr}(\\mathbf x) = -2.58 x_1 + 0.84 x_2 + -3.06\\)",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#training-set",
    "href": "TextClassification/general.html#training-set",
    "title": "Text Classification",
    "section": "Training set",
    "text": "Training set\n\n\n\n\n\n\n\n\n\n\ntext\nklass\n\n\n\n\n0\nEl Depor en los paripes: le ganan al Fuenlabra...\n0\n\n\n1\nCorte Suprema confirma condena de 15 a√±os a Ca...\n1\n\n\n2\n@gusbermudezok Aqu√≠ est√° Tigrotta cuando todav...\n0\n\n\n3\nPOLIC√çAS ESTATALES ASEGURAN GMC SIERRA ROBADA ...\n1\n\n\n4\nAyuda a @PETA_Latino a proteger a los monos, p...\n0\n\n\n5\nUn residente local encar√≥ y persigui√≥ al ataca...\n1\n\n\n6\n@elmostrador Murieron inocentes como en todo c...\n0\n\n\n7\nüö®üöîüöë#Sucesos #SJR Vecinos detienen a sujeto por...\n1",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#quiz",
    "href": "TextClassification/general.html#quiz",
    "title": "Text Classification",
    "section": "Quiz",
    "text": "Quiz\n\n\n\nQuestion\n\n\nWhich of the following tasks does the previous training set belong to?\n\nPolarity\nEmotion identification\nAggressive detection\nProfiling",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#training-set-2",
    "href": "TextClassification/general.html#training-set-2",
    "title": "Text Classification",
    "section": "Training set (2)",
    "text": "Training set (2)\n\n\n\nProblem\n\n\nThe independent variables are texts\n\n\n\n\n\n\n\nSolution\n\n\n\nRepresent the texts in an suitable format for the classifier\nToken as a vector\nSparse vector\nDense vector\nUtterance as a vector",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#token-as-vector",
    "href": "TextClassification/general.html#token-as-vector",
    "title": "Text Classification",
    "section": "Token as Vector",
    "text": "Token as Vector\n\n\n\nToken as vector\n\n\n\nThe idea is that each token \\(t\\) is associate to a vector \\(\\mathbf v_t \\in \\mathbb R^d\\)\nLet \\(\\mathcal V\\) represent the set composed by the different tokens\n\\(d\\) corresponds to the dimension of the vector\n\n\n\n\n\n\n\n\n\\(d &lt;&lt; \\lvert \\mathcal V \\rvert\\) (Dense Vector)\n\n\n\nGloVe\nWord2vec\nfastText",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#token-as-vector-2",
    "href": "TextClassification/general.html#token-as-vector-2",
    "title": "Text Classification",
    "section": "Token as Vector (2)",
    "text": "Token as Vector (2)\n\n\n\n\\(d = \\lvert \\mathcal V \\rvert\\) (Sparse Vector)\n\n\n\n\\(\\forall_{i \\neq j} \\mathbf v_i \\cdot \\mathbf v_j = 0\\)\n\\(\\mathbf v_i \\in \\mathbb R^d\\)\n\\(\\mathbf v_j \\in \\mathbb R^d\\)\n\n\n\n\n\n\n\n\nAlgorithm\n\n\n\nSort the vocabulary \\(\\mathcal V\\)\nAssociate \\(i\\)-th token to\n\\((\\ldots, 0, \\overbrace{\\beta_i}^i, 0, \\ldots)^\\intercal\\)\nwhere \\(\\beta_i &gt; 0\\)",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#utterance-as-vector",
    "href": "TextClassification/general.html#utterance-as-vector",
    "title": "Text Classification",
    "section": "Utterance as Vector",
    "text": "Utterance as Vector\n\n\n\nProcedure\n\n\n\\[\\mathbf x = \\sum_{t \\in \\mathcal U} \\mathbf{v}_t\\]\n\nwhere \\(\\mathcal{U}\\) corresponds to all the tokens of the utterance\nThe vector \\(\\mathbf{v}_t\\) is associated to token \\(t\\)\n\n\n\n\n\n\n\n\nUnit Vector\n\n\n\\[\\mathbf x = \\frac{\\sum_{t \\in \\mathcal U} \\mathbf v_t}{\\lVert \\sum_{t \\in \\mathcal U} \\mathbf v_t \\rVert} \\]",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#tokens",
    "href": "TextClassification/general.html#tokens",
    "title": "Text Classification",
    "section": "Tokens",
    "text": "Tokens\n\n\n\n\n\nflowchart LR\n    Entrada([Text]) --&gt;  Norm[Text Normalizer]\n    Norm --&gt; Seg[Tokenizer]\n    Seg --&gt; Terminos(...)\n\n\n\n\n\n\n\n\n\n\n\nText Normalization\n\n\n\nUser\nURL\nEntity\nCase sensitive\nPunctuation\nDiacritic\n\n\n\n\n\n\n\n\nDiacritic (remove)\n\n\n\ntext = 'M√©xico'\noutput = \"\"\nfor x in unicodedata.normalize('NFD', text):\n    o = ord(x)\n    if 0x300 &lt;= o and o &lt;= 0x036F:\n        continue\n    output += x\noutput\n\n'Mexico'",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#text-normalization-1",
    "href": "TextClassification/general.html#text-normalization-1",
    "title": "Text Classification",
    "section": "Text Normalization",
    "text": "Text Normalization\n\n\n\nCase sensitive\n\n\n\ntext = \"M√©xico\"\noutput = text.lower()\noutput\n\n'm√©xico'\n\n\n\n\n\n\n\n\nURL (replace)\n\n\n\ntext = \"go http://google.com, and find out\"\noutput = re.sub(r\"https?://\\S+\", \"_url\", text)\noutput\n\n'go _url and find out'",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#tokenizer",
    "href": "TextClassification/general.html#tokenizer",
    "title": "Text Classification",
    "section": "Tokenizer",
    "text": "Tokenizer\n\n\n\nCommon Types\n\n\n\nWords\nn-grams (Words)\nq-grams (Characters)\nskip-grams\n\n\n\n\n\n\n\n\nWords\n\n\n\ntext = 'I like playing football on Saturday'\nwords = text.split()\nwords\n\n['I', 'like', 'playing', 'football', 'on', 'Saturday']",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#tokenizer-2",
    "href": "TextClassification/general.html#tokenizer-2",
    "title": "Text Classification",
    "section": "Tokenizer (2)",
    "text": "Tokenizer (2)\n\n\n\n\n\nn-grams\n\n\n\ntext = 'I like playing football on Saturday'\nwords = text.split()\nn = 3\nn_grams = []\nfor a in zip(*[words[i:] for i in range(n)]):\n    n_grams.append(\"~\".join(a))\nn_grams\n\n['I~like~playing',\n 'like~playing~football',\n 'playing~football~on',\n 'football~on~Saturday']\n\n\n\n\n\n\n\n\n\nq-grams\n\n\n\ntext = 'I like playing'\nq = 4\nq_grams = []\nfor a in zip(*[text[i:] for i in range(q)]):\n    q_grams.append(\"\".join(a))\nq_grams\n\n['I li',\n ' lik',\n 'like',\n 'ike ',\n 'ke p',\n 'e pl',\n ' pla',\n 'play',\n 'layi',\n 'ayin',\n 'ying']",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#mu-tc",
    "href": "TextClassification/general.html#mu-tc",
    "title": "Text Classification",
    "section": "\\(\\mu\\)-TC",
    "text": "\\(\\mu\\)-TC\n\n\n\nTextModel\n\n\n\ntm = TextModel(token_list=[-1],\n               num_option=OPTION_NONE,\n               usr_option=OPTION_DELETE,\n               url_option=OPTION_DELETE,\n               emo_option=OPTION_NONE,\n               lc=True,\n               del_dup=False,\n               del_punc=True,\n               del_diac=True)\n\n\n\n\n\n\ntext = 'I like playing football with @mgraffg'\ntm.tokenize(text)\n\n['i', 'like', 'playing', 'football', 'with']",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#mu-tc-2",
    "href": "TextClassification/general.html#mu-tc-2",
    "title": "Text Classification",
    "section": "\\(\\mu\\)-TC (2)",
    "text": "\\(\\mu\\)-TC (2)\n\n\n\n\n\nTextModel\n\n\n\ntm = TextModel(token_list=[-2, -1, 6],\n               num_option=OPTION_NONE,\n               usr_option=OPTION_DELETE,\n               url_option=OPTION_DELETE,\n               emo_option=OPTION_NONE, \n               lc=True, del_dup=False,\n               del_punc=True, del_diac=True)\n\n\n\n\n\n\ntext = 'I like playing...'\ntm.tokenize(text)\n\n['i~like',\n 'like~playing',\n 'i',\n 'like',\n 'playing',\n 'q:~i~lik',\n 'q:i~like',\n 'q:~like~',\n 'q:like~p',\n 'q:ike~pl',\n 'q:ke~pla',\n 'q:e~play',\n 'q:~playi',\n 'q:playin',\n 'q:laying',\n 'q:aying~']",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#training-set-1",
    "href": "TextClassification/general.html#training-set-1",
    "title": "Text Classification",
    "section": "Training set",
    "text": "Training set\n\nURL = 'https://github.com/INGEOTEC/Delitos/releases/download/Datos/delitos.zip'\nif not isfile('delitos.zip'):\n  Download(URL,\n           'delitos.zip')\nif not isdir('delitos'):\n  !unzip -Pingeotec delitos.zip",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#utterance-as-vector-1",
    "href": "TextClassification/general.html#utterance-as-vector-1",
    "title": "Text Classification",
    "section": "Utterance as Vector",
    "text": "Utterance as Vector\n\n\n\n\n\nTextModel\n\n\n\ntm = TextModel(token_list=[-1],\n               num_option=OPTION_NONE,\n               usr_option=OPTION_DELETE,\n               url_option=OPTION_DELETE,\n               emo_option=OPTION_NONE, \n               lc=True, del_dup=False,\n               del_punc=True, del_diac=True)\n\n\n\n\n\n\n\n\nTokenizer\n\n\n\nfname = 'delitos/delitos_ingeotec_Es_train.json'\ntraining_set = list(tweet_iterator(fname))\ntm.tokenize(training_set[0])[:3]\n\n['este', 'caso', 'tiene']\n\n\n\n\n\n\n\n\n\n\nVocabulary\n\n\n\nvoc = Counter()\nfor text in training_set:\n  tokens = set(tm.tokenize(text))\n  voc.update(tokens)\nvoc.most_common(n=3)\n\n[('de', 980), ('en', 803), ('la', 653)]",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#utterance-as-vector-2",
    "href": "TextClassification/general.html#utterance-as-vector-2",
    "title": "Text Classification",
    "section": "Utterance as Vector (2)",
    "text": "Utterance as Vector (2)\n\n\n\nInverse Document Frequency (IDF)\n\n\n\ntoken2id = {}\ntoken2beta = {}\nN = np.log2(voc.update_calls)\nfor id, (k, n) in enumerate(voc.items()):\n  token2id[k] = id\n  token2beta[k] = N - np.log2(n)\n\n\n\n\n\n\n\n\nTerm Frequency - IDF\n\n\n\ntext = training_set[3]['text']\ntokens = tm.tokenize(text)\nvector = []\nfor token, tf in zip(*np.unique(tokens, return_counts=True)):\n  if token not in token2id:\n    continue\n  vector.append((token2id[token], tf * token2beta[token]))\nvector[:4]\n\n[(57, np.float64(7.906890595608518)),\n (51, np.float64(5.8479969065549495)),\n (0, np.float64(1.3269461696539864)),\n (9, np.float64(2.5277907564370796))]",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#utterance-as-vector-3",
    "href": "TextClassification/general.html#utterance-as-vector-3",
    "title": "Text Classification",
    "section": "Utterance as Vector (3)",
    "text": "Utterance as Vector (3)\n\n\n\n\\(\\mu\\)-TC\n\n\n\ntm.fit(training_set)\n\n&lt;microtc.textmodel.TextModel at 0x7efc0b2cbca0&gt;\n\n\n\n\n\n\n\n\nUtterance as Vector\n\n\n\ntext = training_set[3]['text']\ntm[text][:4]\n\n[(3535, np.float64(0.08495635021337841)),\n (5135, np.float64(0.0766897986795882)),\n (6598, np.float64(0.3526199879358128)),\n (3350, np.float64(0.19654493631439243))]",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#quiz-1",
    "href": "TextClassification/general.html#quiz-1",
    "title": "Text Classification",
    "section": "Quiz",
    "text": "Quiz\n\n\n\nQuestion\n\n\nWhich of the following representations do you consider to produce a larger vocabulary?\n\n\n\n\n\n\n\n\nA\n\n\n\ntmA = TextModel(token_list=[-1, 3],\n                num_option=OPTION_NONE,\n                usr_option=OPTION_DELETE,\n                url_option=OPTION_DELETE,\n                emo_option=OPTION_NONE, \n                lc=True, del_dup=False,\n                del_punc=True,\n                del_diac=True\n               ).fit(training_set)\n\n\n\n\n\n\n\n\nB\n\n\n\ntmB = TextModel(token_list=[-1, 6],\n                num_option=OPTION_NONE,\n                usr_option=OPTION_DELETE,\n                url_option=OPTION_DELETE,\n                emo_option=OPTION_NONE, \n                lc=True, del_dup=False,\n                del_punc=True,\n                del_diac=True\n               ).fit(training_set)",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#procedure-1",
    "href": "TextClassification/general.html#procedure-1",
    "title": "Text Classification",
    "section": "Procedure",
    "text": "Procedure\n\n\n\nText as Vectors\n\n\n\ntm = TextModel(token_list=[-1], num_option=OPTION_NONE,\n                usr_option=OPTION_DELETE, url_option=OPTION_DELETE,\n                emo_option=OPTION_NONE, lc=True, del_dup=False,\n                del_punc=True, del_diac=True\n               ).fit(training_set)\nX = tm.transform(training_set)\n\n\n\n\n\n\n\n\nTraining a Classifier\n\n\n\nlabels = [x['klass'] for x in training_set]\nm = LinearSVC(dual='auto').fit(X, labels)\n\n\n\n\n\n\n\n\n\nPredict a text\n\n\n\nX = tm.transform(['Buenos d√≠as']) # good morning\nm.predict(X)\n\narray([0])",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#performance",
    "href": "TextClassification/general.html#performance",
    "title": "Text Classification",
    "section": "Performance",
    "text": "Performance\n\n\n\nTest set\n\n\n\ntest_set = list(tweet_iterator(fname.replace('_train.', '_test.')))\n\n\n\n\n\n\n\nPrediction\n\n\n\ntm = TextModel(token_list=[-2, -1, 3, 4], num_option=OPTION_NONE,\n               usr_option=OPTION_DELETE, url_option=OPTION_DELETE,\n               emo_option=OPTION_NONE, lc=True, del_dup=False,\n               del_punc=True, del_diac=True\n              ).fit(training_set)\nX = tm.transform(training_set)\nlabels = np.array([x['klass'] for x in training_set])\nm = LinearSVC(dual='auto', class_weight='balanced').fit(X, labels)\nhy = m.predict(tm.transform(test_set))\n\n\n\n\n\n\n\nPerformance\n\n\n\nrecall_score([x['klass'] for x in test_set],\n             hy, average=None)\n\narray([0.95070423, 0.68421053])",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#feature-importance",
    "href": "TextClassification/general.html#feature-importance",
    "title": "Text Classification",
    "section": "Feature Importance",
    "text": "Feature Importance\n\n\n\nCoefficients\n\n\n\ndef coef(X, y):\n    m = LinearSVC(dual='auto',\n                  class_weight='balanced'\n                 ).fit(X, y)\n    return m.coef_\n\n\n\n\n\n\n\nNormalize Coefficients\n\n\n\nstats = StatisticSamples(statistic=coef,\n                         num_samples=50,\n                         n_jobs=-1)\nb_samples = stats(X, labels)\nse = np.std(b_samples, axis=0)\nse[se==0] = 1\nw_norm = m.coef_ / se\nw_norm = np.linalg.norm(w_norm, axis=0)",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#feature-importance-2",
    "href": "TextClassification/general.html#feature-importance-2",
    "title": "Text Classification",
    "section": "Feature Importance (2)",
    "text": "Feature Importance (2)\n\n\n\nWordcloud\n\n\n\npath = './emoji_text.ttf'\nitems = tm.token_weight.items\ntokens = {tm.id2token[id]: w_norm[id] * _w for id, _w in items()\n          if w_norm[id] &gt;= 2.0 and np.isfinite(w_norm[id])}\nword_cloud = WordCloud(font_path=path,\n                       background_color='white'\n                      ).generate_from_frequencies(tokens)\nplt.imshow(word_cloud, interpolation='bilinear')\nplt.tick_params(left=False, right=False, labelleft=False,\n                   labelbottom=False, bottom=False)",
    "crumbs": [
      "Text Classification"
    ]
  },
  {
    "objectID": "TextClassification/general.html#feature-importance-2-output",
    "href": "TextClassification/general.html#feature-importance-2-output",
    "title": "Text Classification",
    "section": "Feature Importance (2)",
    "text": "Feature Importance (2)",
    "crumbs": [
      "Text Classification"
    ]
  }
]